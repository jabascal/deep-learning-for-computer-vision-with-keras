{"cells":[{"cell_type":"markdown","metadata":{},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jabascal/deep-learning-for-computer-vision-with-keras/blob/main/image_classification_using_transfer_learning_ensembles.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"3Gl9vcdgAois"},"source":["# Image classification using transfer learning and ensemble methods"]},{"cell_type":"markdown","metadata":{"id":"RzBtnMxxAuRR"},"source":["The objectives of this notebook are to create a classifier that leads to the largest accuracy on the given dataset using a transfer learning and fine-tuning approach. Then, ensemble method is used aiming at a final improvement in performance. \n","\n","For *transfer learning*, we use the pretrained weights of an architecture pretrained on a large-scale dataset (ImageNet), excluding the top classification layer. Then, we freeze the rest of layers and add a classification layer adapted to the number of classes in our specific dataset. Finally, we train the new model, which has a very low number of weights (few thousands) compared to  the base model (with several millions), on a specific and smaller dataset. \n","\n","For *fine tuning*, we unfreeze few or all of the top layers of the base model and then tune the weights to obtain higher accuracy. \n","\n","For *ensemble methods*, several instances of the same model are trained and then are combined by majority vote."]},{"cell_type":"markdown","metadata":{"id":"GoEHZu2HEj5o"},"source":["## Import dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mode_install = False\n","if mode_install:\n","    !pip install tensorflow \\\n","        pillow \\\n","        matplotlib \\\n","        pandas \\\n","        scikit-learn"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1445,"status":"ok","timestamp":1622872772016,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"855u4ZVHExpn","outputId":"5366b503-7657-49d8-a6cf-eb173954c82a"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","print(tf.__version__)\n","\n","import pathlib\n","import random\n","import time\n","import PIL\n","import json"]},{"cell_type":"markdown","metadata":{"id":"KJp8MJqBOM9-"},"source":["To access **GPU** go to 'Runtime/Change runtime type' and to check if GPU is available and resources, run the following code:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":241,"status":"ok","timestamp":1622872780376,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"oroBrZJEcOPG","outputId":"3257f4b8-fd50-48c1-c0d6-fdbb8f44c3bc"},"outputs":[],"source":["# Device name\n","tf.test.gpu_device_name()\n","\n","# GPU (Tesla), memory limit (14GB)\n","from tensorflow.python.client import device_lib\n","device_lib.list_local_devices()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mFTM28s3dzPE"},"outputs":[],"source":["# Memory resources\n","!cat /proc/meminfo"]},{"cell_type":"markdown","metadata":{"id":"43yGecu0hQ2J"},"source":["## Download data"]},{"cell_type":"markdown","metadata":{"id":"WduIJ2VUhfrx"},"source":["### Mount google drive and paths"]},{"cell_type":"markdown","metadata":{"id":"o80j4-WHFpm9"},"source":["Start by mounting your google drive:   "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15961,"status":"ok","timestamp":1622872811787,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"bVqNrmTrFnkU","outputId":"98835e9a-59e9-49e9-aab2-612ee07e6adf"},"outputs":[],"source":["# Mount google drive to access files via colab\n","mode_colab = False\n","if mode_colab:\n","    from google.colab import drive\n","    drive.mount(\"/content/gdrive\")"]},{"cell_type":"markdown","metadata":{"id":"yx4SH1RDG1tc"},"source":["Specify the path of the notebook, something like /content/gdrive/MyDrive/deep-learning-for-computer-vision-with-keras/codetest/ (clik on the link to open the contents on the left pannel), and a path to save results. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["name_save = \"ensembles\"\n","\n","# Path results\n","if mode_colab:\n","      save_dir = \"/content/gdrive/MyDrive/Colab_Notebooks/Results/ensembles\"\n","      data_dir = '/content/gdrive/MyDrive/Colab_Notebooks/Data'\n","else:\n","      save_dir = \"../../Results/ensembles\"\n","      data_dir = '../../Data'\n","if os.path.exists(save_dir) is False:\n","      os.mkdir(save_dir)\n","      print(f\"Directory: {save_dir} created.\")\n","if os.path.exists(data_dir) is False:\n","      os.mkdir(data_dir)\n","      print(f\"Directory: {data_dir} created.\")"]},{"cell_type":"markdown","metadata":{"id":"ht2znQiXE1aU"},"source":["### Download data"]},{"cell_type":"markdown","metadata":{"id":"groP5H2yJwQz"},"source":["Download the data subset automatically into your drive. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"102-nO6IFvpH"},"outputs":[],"source":["dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n","data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True, cache_dir=data_dir)\n","print(f\"Data downloaded to {os.path.abspath(data_dir)}\")\n","!rm \"{data_dir}/../flower_photos.tar.gz\""]},{"cell_type":"markdown","metadata":{"id":"BlUOEBt9E7Ck"},"source":["## Define parameters and general functions"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":203,"status":"ok","timestamp":1622872869113,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"6TrtAduSFAD0"},"outputs":[],"source":["# Image sizes\n","batch_size = 32\n","img_height = 180\n","img_width = 180\n","\n","learning_rate = 1e-4"]},{"cell_type":"markdown","metadata":{"id":"e0NM7vVnhuVB"},"source":["## Data pipeline"]},{"cell_type":"markdown","metadata":{"id":"DoKQLR9MFOi-"},"source":["### Create dataset"]},{"cell_type":"markdown","metadata":{"id":"5L2tRTMZFaPf"},"source":["We use the tensorflow data API to automatize the data pipeline, chaining transformations (preprocessing and data augmentation), shuffling data. \n","\n","Next, we create dataset using 'image_dataset_from_directory' to get similar labeled dataset objects to specified folders. Split data into train, validation and test. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":658,"status":"ok","timestamp":1622872881021,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"OoOYqHnOFV7_","outputId":"d54a8d05-ed58-46cf-aae1-ede1494b500f"},"outputs":[],"source":["# Create data set \n","# Split in training and validation\n","train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","  data_dir,\n","  validation_split=0.2,\n","  subset=\"training\",\n","  seed=123,\n","  image_size=(img_height, img_width),\n","  batch_size=batch_size,\n","  shuffle=True)\n","\n","val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","  data_dir,\n","  validation_split=0.2,\n","  subset=\"validation\",\n","  seed=123,\n","  image_size=(img_height, img_width),\n","  batch_size=batch_size,\n","  shuffle=True)\n","\n","# Class names\n","class_names = train_ds.class_names\n","num_classes = len(class_names)\n","print('Classes names: ')\n","print(class_names)\n","\n","# Split Validation and Test\n","val_batch_size = val_ds.cardinality().numpy()\n","test_ds = val_ds.take(int(0.5*val_batch_size))\n","val_ds = val_ds.skip(int(0.5*val_batch_size))\n"]},{"cell_type":"markdown","metadata":{"id":"CLqO7PfUH75Y"},"source":["An efficient pipeline can be obtained using 'cache' which keeps the data in RAM memory after the first epoch and 'prefetch' which allows to prepare data for next batch while the model is being trained for the current batch on the GPU. Data is shuffled at each iteration for training data."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1622872881022,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"FFJcXyCNH6vu"},"outputs":[],"source":["AUTOTUNE = tf.data.AUTOTUNE\n","\n","# shuffle after cache\n","train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n","val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"]},{"cell_type":"markdown","metadata":{"id":"lWkmkafBF2FB"},"source":["### Data visualization "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":591},"executionInfo":{"elapsed":5483,"status":"ok","timestamp":1622872898055,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"fsxxN3PoF6Ie","outputId":"69e80fe2-fab2-41a2-eb60-11fa4906fe56"},"outputs":[],"source":["# Display several images\n","fig = plt.figure(figsize=(10, 10))\n","for images, labels in train_ds.take(1):\n","  for i in range(9):\n","    ax = plt.subplot(3, 3, i + 1)\n","    plt.imshow(images[i].numpy().astype(\"uint8\"))\n","    plt.title(class_names[labels[i]])\n","    plt.axis(\"off\")\n","    fig.savefig(os.path.join(save_dir, f\"{name_save}_grid.png\"), bbox_inches='tight', dpi=300)"]},{"cell_type":"markdown","metadata":{},"source":["Display an image per class"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Display an image per class\n","fig, axs = plt.subplots(1, num_classes, figsize=(15, 5))\n","for i, class_name in enumerate(class_names):\n","    # Get an image from the class\n","    image_path = os.path.join(data_dir, class_name, os.listdir(os.path.join(data_dir, class_name))[0])\n","    image = PIL.Image.open(image_path)\n","    \n","    # Display the image\n","    axs[i].imshow(image)\n","    axs[i].set_title(class_name)\n","    axs[i].axis('off')\n","\n","plt.show()\n","fig.savefig(os.path.join(save_dir, f\"{name_save}_classes.png\"), bbox_inches='tight', dpi=300)\n"]},{"cell_type":"markdown","metadata":{"id":"JbS_R3gtHdIf"},"source":["Images contain several objects and different background, which may harden the classification task."]},{"cell_type":"markdown","metadata":{"id":"OLAdRKnwG7GF"},"source":["Number of data per class:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":281},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1622872898056,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"stcweBYtG5sF","outputId":"5affcdf1-46e5-4699-ad22-e2bd8d6fb174"},"outputs":[],"source":["# Number of samples per class\n","data_dir = pathlib.Path(data_dir)\n","\n","class_counts = []\n","for class_name in class_names:\n","    class_count = len(list(data_dir.glob(class_name+'/*.jpg')))\n","    class_counts.append(class_count)\n","\n","fig = plt.figure(figsize=(4,4))\n","plt.barh(class_names, class_counts)\n","plt.title('Number per class')\n","plt.show()\n","fig.savefig(os.path.join(save_dir, f\"{name_save}_classes_counts.png\"), bbox_inches='tight', dpi=300)"]},{"cell_type":"markdown","metadata":{"id":"3I-ngUdIJGl6"},"source":["### Data augmentation"]},{"cell_type":"markdown","metadata":{"id":"uU1Y_Ts2glJM"},"source":["Data augmentation is performed using random flip,  rotation and zooming. Many more operataions can be used, such as random contrast, brightness, hue, saturation, etc. See [tf.image](https://www.tensorflow.org/api_docs/python/tf/image/) for more details."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":411},"executionInfo":{"elapsed":1311,"status":"ok","timestamp":1622872906471,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"E8tpOfM2JJvp","outputId":"ea227ca1-fc40-480f-b51c-3c149d5acc71"},"outputs":[],"source":["# Data augmentation\n","data_augmentation = keras.Sequential(\n","  [\n","    layers.RandomFlip(\"horizontal\", \n","                                                 input_shape=(img_height, \n","                                                              img_width,\n","                                                              3)),\n","    layers.RandomRotation(0.1),\n","    layers.RandomZoom(0.1),\n","  ]\n",")\n","\n","# Data augmentation example\n","fig = plt.figure(figsize=(7, 7))\n","for images, _ in train_ds.take(1):\n","  for i in range(9):\n","    augmented_images = data_augmentation(images)\n","    ax = plt.subplot(3, 3, i + 1)\n","    plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n","    plt.axis(\"off\")\n","fig.savefig(os.path.join(save_dir, f\"{name_save}_augmentation.png\"), bbox_inches='tight', dpi=300)"]},{"cell_type":"markdown","metadata":{"id":"YQFibRaAxo8M"},"source":["Callback for early stopping:"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":204,"status":"ok","timestamp":1622873743396,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"oEG6bffRMutC"},"outputs":[],"source":["# Early stopping based on a given metric\n","earlystop_cb = keras.callbacks.EarlyStopping(\n","    patience=10, \n","    monitor='val_accuracy',\n","    restore_best_weights=True)\n","\n","callbacks = [earlystop_cb]\n"]},{"cell_type":"markdown","metadata":{"id":"FpUQ8SZCyPO_"},"source":["Loading function to load test set for model evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1370,"status":"ok","timestamp":1622873747561,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"RcsIWRTCSw-F"},"outputs":[],"source":["# Load test data from test dataset. \n","def get_imgs_from_dataset(ds_test, ds_test_size):\n","    # Take images from data set ds_test: (data_test, data_test_noisy) \n","    data_test = []\n","    label_test = []\n","    count = 0\n","    for img, label in ds_test.take(ds_test_size):\n","        data_test_this = img.numpy()  \n","        label_test_this = label.numpy() \n","        if count == 0:\n","            data_test = data_test_this\n","            label_test = label_test_this\n","            count = 1\n","        else:            \n","            data_test = np.append(data_test, data_test_this, axis=0)\n","            label_test = np.append(label_test, label_test_this, axis=0)\n","    return data_test, label_test\n","    \n","# Test data\n","data_test, label_test = get_imgs_from_dataset(val_ds, len(val_ds)-1)"]},{"cell_type":"markdown","metadata":{"id":"uVvh9ZmfJhGo"},"source":["## Model, training and assessment: A transfer learning with fine tuning approach followed by ensemble voting"]},{"cell_type":"markdown","metadata":{"id":"h9s-9Yh0SG7-"},"source":["### Create the model"]},{"cell_type":"markdown","metadata":{"id":"i2WoPwN9JnLy"},"source":["We use transfer learning using a pretrained model that has been trained on a very large dataset (ImageNet). We try to different models: 'Xception' which provides a high top-5 accuracy (with 20 M parameters) and 'MobileNetV2' which provides great accuracy for a relatively small model size. \n","\n","We load the model but skip the 'top' layer to tailored our model to the classes in the dataset. Then, we freeze their layers to train on a small dataset. We also define the model and specify their preprocessing steps. "]},{"cell_type":"markdown","metadata":{"id":"loiGuh-QR_8g"},"source":["Instead of creating and training the model, you can load the trained model: run the code below."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_base_model(base_model_name, input_shape):\n","    # Download pretrained base model \n","    if base_model_name == 'mobilenet_v2':\n","        # mobilenet_v2: small networks\n","        # Param M: 4.24, top-1 acc: 70.9, top-5 acc:\t89.9\n","        # Imagenet ILSVRC-2012-CLS \n","        # Timing: # GTX: 7s and 3s, FT: \n","        base_model = tf.keras.applications.MobileNetV2(\n","                input_shape=input_shape,                                                   \n","                include_top=False,                                                   \n","                weights='imagenet')\n","        preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n","    elif base_model_name == 'mobilenet_v3':    \n","        # mobilenet_v3: small networks\n","        base_model = tf.keras.applications.MobileNetV3Small(\n","                input_shape=input_shape,                                                   \n","                include_top=False,                                                   \n","                weights='imagenet')\n","        preprocess_input = tf.keras.applications.mobilenet_v3.preprocess_input\n","    elif base_model_name == 'EfficientNetB0':\n","        # EfficientNetB0: 5M\n","        base_model = keras.applications.EfficientNetB0(\n","                input_shape=input_shape,                                                   \n","                include_top=False,                                                   \n","                weights='imagenet')\n","        preprocess_input = tf.keras.applications.efficientnet.preprocess_input    \n","    elif base_model_name == 'EfficientNetB4':\n","        # EfficientNetB4\n","        base_model = keras.applications.EfficientNetB4(\n","                input_shape=input_shape,                                                   \n","                include_top=False,                                                   \n","                weights='imagenet')\n","        preprocess_input = tf.keras.applications.efficientnet.preprocess_input\n","    elif base_model_name == 'EfficientNetB7':\n","        # EfficientNetB7\n","        base_model = keras.applications.EfficientNetB7(\n","                input_shape=input_shape,                                                   \n","                include_top=False,                                                   \n","                weights='imagenet')\n","        preprocess_input = tf.keras.applications.efficientnet.preprocess_input\n","    elif base_model_name == 'Xception':\n","        # Xception\n","        # 20 M parameters\n","        # no smaller than 71. E.g. (150, 150, 3) \n","        # Timing: # GTX: 10s and 6s, FT: 15s and 9s\n","        base_model = keras.applications.Xception(\n","                input_shape=input_shape,                                                   \n","                include_top=False,                                                   \n","                weights='imagenet')\n","        preprocess_input = tf.keras.applications.xception.preprocess_input\n","    elif base_model_name == 'ResNet50':\n","        # ResNet50: 25 M\n","        base_model = keras.applications.ResNet50(\n","                input_shape=input_shape,                                                   \n","                include_top=False,                                                   \n","                weights='imagenet')\n","        preprocess_input = tf.keras.applications.resnet50.preprocess_input\n","    elif base_model_name == 'vgg19':\n","        # VGG19: 143 M \n","        base_model = keras.applications.VGG19(\n","                input_shape=input_shape,                                                   \n","                include_top=False,                                                   \n","                weights='imagenet')\n","        preprocess_input = tf.keras.applications.vgg19.preprocess_input\n","    elif base_model_name == 'inception_v3':\n","        # InceptionV3: 23M\n","        base_model = keras.applications.InceptionV3(\n","                input_shape=input_shape,                                                   \n","                include_top=False,                                                   \n","                weights='imagenet')\n","        preprocess_input = tf.keras.applications.inception_v3.preprocess_input\n","    \n","    # Freeze weights\n","    base_model.trainable = False\n","    return base_model, preprocess_input"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_model(inputs_shape, preprocess_input, base_model, \n","              num_classes=num_classes, dropout_rate=0.2):\n","\n","    # Average pooling layer to pass from block 6x6x1280 to vector 1x1280\n","    global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n","\n","    # Multiclass classification layer\n","    prediction_layer = tf.keras.layers.Dense(num_classes)\n","\n","    # Create the model\n","    inputs = tf.keras.Input(shape=inputs_shape)\n","    x = data_augmentation(inputs)\n","    x = preprocess_input(x)        \n","    x = base_model(x, training=False)\n","    x = global_average_layer(x)\n","    x = layers.Dropout(dropout_rate)(x)\n","    outputs = prediction_layer(x)\n","    model = tf.keras.Model(inputs, outputs)\n","\n","    model.summary()\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def freeze_layers(base_model, fine_tune_at):    \n","    # Unfreeze the base model\n","    base_model.trainable = True\n","\n","    # Number layers are in the base model\n","    print(\"Number of layers in the base model: \", len(base_model.layers))\n","\n","    # Freeze all the layers before the `fine_tune_at` layer\n","    for layer in base_model.layers[:fine_tune_at]:\n","        layer.trainable =  False\n","    return base_model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def display_training_curves(acc, val_acc, loss, val_loss, name_save):\n","  epochs_range = range(len(acc))\n","\n","  fig = plt.figure(figsize=(8, 8))\n","  plt.subplot(1, 2, 1)\n","  plt.plot(epochs_range, acc, label='Training Accuracy')\n","  plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n","  plt.legend(loc='lower right')\n","  plt.title('Training and Validation Accuracy')\n","  #\n","  plt.subplot(1, 2, 2)\n","  plt.plot(epochs_range, loss, label='Training Loss')\n","  plt.plot(epochs_range, val_loss, label='Validation Loss')\n","  plt.legend(loc='upper right')\n","  plt.title('Training and Validation Loss')\n","  plt.show()\n","  fig.savefig(name_save, bbox_inches='tight', dpi=300)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Loss, metrics, optimizer\n","acc_fn = tf.keras.metrics.Accuracy()\n","loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","metrics = ['accuracy']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"CzHyjgKvKJWc","outputId":"b0b936b7-1786-4313-9521-594a19972240"},"outputs":[],"source":["# Number of models for ensemble voting\n","IMG_SHAPE = (img_height, img_width) + (3,)\n","fine_tune_at = 70     # Fine-tune from this layer onwards\n","epochs = 100          # 100\n","fine_tune_epochs = 70 #70\n","\n","base_model_name_list = ['mobilenet_v2', 'mobilenet_v3', 'EfficientNetB0']\n","test_acc_models = []\n","models = []\n","for base_model_name in base_model_name_list:\n","  # Define pretrained base model\n","  print('*'*50)\n","  print(f\"Base model: {base_model_name}\") \n","  base_model, preprocess_input = get_base_model(base_model_name, IMG_SHAPE)\n","\n","  # Model name\n","  model = get_model(IMG_SHAPE, preprocess_input, base_model)\n","\n","  # Compile the model\n","  optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","  model.compile(optimizer=optimizer,\n","                loss=loss_fn,\n","                metrics=metrics)\n","\n","  # Train a large number of steps with early stopping criterion\n","  history = model.fit(train_ds, validation_data=val_ds, epochs=epochs, \n","                      callbacks=callbacks, verbose = 0)\n","\n","  # Display loss and accuracy\n","  acc = history.history['accuracy']\n","  val_acc = history.history['val_accuracy']\n","  loss = history.history['loss']\n","  val_loss = history.history['val_loss']\n","\n","  # FINE TUNING      \n","  base_model = freeze_layers(base_model, fine_tune_at)\n","\n","  # Recompile your model after you make any changes\n","  optimizer_ft = tf.keras.optimizers.RMSprop(learning_rate=learning_rate/10)\n","  model.compile(optimizer = optimizer_ft,  # Lower learning rate\n","                loss=loss_fn,\n","                metrics=metrics)\n","  # ---------------------------------------------------------------------\n","  # Train (continue training)\n","  total_epochs = epochs + fine_tune_epochs    \n","  history_fine = model.fit(train_ds,\n","                            epochs=total_epochs,\n","                            initial_epoch=history.epoch[-1],\n","                            validation_data=val_ds,\n","                            callbacks=callbacks,\n","                           verbose=0)\n","\n","  # Display losses\n","  acc += history_fine.history['accuracy']\n","  val_acc += history_fine.history['val_accuracy']\n","\n","  loss += history_fine.history['loss']\n","  val_loss += history_fine.history['val_loss']\n","\n","  epochs_range = range(len(acc))\n","\n","  name_save_this = os.path.join(save_dir, f\"{name_save}_{base_model_name}_loss_ep{epochs}_ft{fine_tune_epochs}.png\")\n","  display_training_curves(acc, val_acc, loss, val_loss, name_save)\n","\n","  # Predict \n","  data_pred = model.predict(data_test)\n","  data_pred_class = np.argmax(data_pred, axis=1)\n","\n","  test_acc = acc_fn(data_pred_class, label_test)\n","  print('Accuracy for %s with fine tuning is %.2f' % (base_model_name, test_acc))\n","  test_acc_models.append(test_acc)\n","\n","  # Save the model in keras format (default).\n","  name_save = 'flower_photos_' + base_model_name + '_TF' + str(history.epoch[-1]) +'it' + '_FTat' + str(fine_tune_at) + '_it' + str(history_fine.epoch[-1]) + '_Ens' + str(i_ensemble) \n","  model.save(os.path.join(save_dir, name_save + \".keras\"))\n","  print(f\"Model saved in {os.path.abspath(os.path.join(save_dir, name_save))}\")\n","  models.append(model)  \n","\n","  # Save history\n","  with open(os.path.join(save_dir, name_save + '.json'), 'w') as file:\n","    json.dump(history_fine.history, file)"]},{"cell_type":"markdown","metadata":{"id":"T_Vztz2sz13t"},"source":["### Ensemble assessment "]},{"cell_type":"markdown","metadata":{},"source":["We combined the individually trained models by majority voting."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def ensemble_voting(data_pred_list):\n","    # Voting\n","    data_pred_ensemble = np.mean(data_pred_list, axis=0)\n","    data_pred_class_ensemble = np.argmax(data_pred_ensemble, axis=1)\n","    return data_pred_class_ensemble"]},{"cell_type":"markdown","metadata":{},"source":["Another option is to create a combined ensembled model and train it, based on this [blog article](https://blog.paperspace.com/ensembling-neural-network-models/). "]},{"cell_type":"markdown","metadata":{},"source":["### Model assessment"]},{"cell_type":"markdown","metadata":{},"source":["Alternatively, we can load the data, predict and compute the desired metric."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load test data from test dataset. \n","def get_imgs_from_dataset(ds_test, ds_test_size):\n","    # Take images from data set ds_test: (data_test, data_test_noisy) \n","    data_test = []\n","    label_test = []\n","    count = 0\n","    for img, label in ds_test.take(ds_test_size):\n","        data_test_this = img.numpy()  \n","        label_test_this = label.numpy() \n","        if count == 0:\n","            data_test = data_test_this\n","            label_test = label_test_this\n","            count = 1\n","        else:            \n","            data_test = np.append(data_test, data_test_this, axis=0)\n","            label_test = np.append(label_test, label_test_this, axis=0)\n","    return data_test, label_test\n","    \n","# Test data\n","data_test, label_test = get_imgs_from_dataset(val_ds, len(val_ds)-1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Predict for all models\n","data_pred_list = []\n","test_acc_models = []\n","for model in models:\n","    data_pred = model.predict(data_test)\n","    data_pred_list.append(data_pred)\n","    data_pred_class = np.argmax(data_pred, axis=1)\n","    test_acc = acc_fn(data_pred_class, label_test)\n","    test_acc_models.append(test_acc)\n","\n","print('Accuracy for each model is: ', test_acc_models)\n","\n","# Ensemble voting\n","data_pred_class_ensemble = ensemble_voting(data_pred_list)\n","test_acc_ensemble = acc_fn(data_pred_class_ensemble, label_test)\n","print('Accuracy for ensemble is %.2f' % test_acc_ensemble)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPj+PbjPsaxuOemUQ6Z8/Jv","collapsed_sections":["IwoYfJ9ntIhm","am_XRmBBfwWb"],"name":"image_classification_using_transfer_learning_ensembles.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
