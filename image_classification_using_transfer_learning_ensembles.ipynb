{"cells":[{"cell_type":"markdown","metadata":{},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jabascal/deep-learning-for-computer-vision-with-keras/blob/main/image_classification_using_transfer_learning_ensembles.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"3Gl9vcdgAois"},"source":["# Image classification using transfer learning and ensemble methods"]},{"cell_type":"markdown","metadata":{},"source":["Emsemble learning combine multiple models through a voting mechanism in order to improve generalization performance [Ganaie 2022]. It has been shown that if the probability of individual voters is larger than 0.5 and the voters are independent, adding more voters will increase the probability of the ensemble model. They have been widely used in image classification and healthcare.\n","\n","This can be explained by bias-variance decomposition, statistical computational theory and representation and diversity. \n","- As shown in the appendix, the ambiguity decomposition separates the variance of each model and the covariance between models. \n","- From statistical theory, the learning model can be viewed as an optimal hyopthesis among the space of all possible hypotheses.The ensemble model reduces the risk of selecting a wrong classifier. \n","- From the computational point of view, the ensemble model is more robust to local minima. \n","- From the representation point of view, the ensemble model augments the representational capacity of the model.\n","\n","How to induce diversity among the and how to fuse the outputs are key issues. For instance, 'output smearing' adds noise to the output of the model to induce diversity. Ensembling either combines the votes or selects the best base model\n","\n","There are different ensemble strategies: bagging, boosting, stacking, and voting. \n","- In bagging (bootstrap aggregating), a series of independent observations are drawn from the training set to train the different models and then outputs are combined with majority voting, for classification, or averaging, for regression. Random Forest is an example of bagging that uses decision trees as base learners. \n","\n","- Boosting converts weak learners into strong learners by training the models sequentially. AdaBoost and Gradient Boosting are examples of boosting. AdaBoost leverages the misclassification of the previous model to train the next model. \n","\n","- Stacking is a bias reduction technique that concatenates the outputs of the base models and trains a meta-model on top of them. \n"]},{"cell_type":"markdown","metadata":{},"source":["Combining several trained deep NNs can be computationally expensive. Ensembles can be implicit or explicit. \n","- In implicit ensembles, parameters are shared among the models and the models are trained simultaneously. Dropout is used to combine the models, which avoids overfitting and introduces sparsity in the output vectors. Negative correlation is a technique for training the models in such a way that the errors of the models are uncorrelated.  \n","- In explicit ensembles, the models are trained independently and then combined.\n","\n","Ensemble models can be homogeneous or heterogeneous. In homogeneous ensembles, the models are of the same type, and are usually trained in such a way that they are diverse, sampling the trainig data or the feature space. Complex heterogeneous ensembles, combining different types of models such as SVM, random forest, CNN, and others can lead to better generalization. "]},{"cell_type":"markdown","metadata":{},"source":["In this notebook, we show how to use transfer learning and ensemble methods to improve the performance of image classification on the flowers dataset. First, we use explicit ensembles where different standard models are trained independently and then combined. Then, we use an implicit approach where the models are trained simultaneously."]},{"cell_type":"markdown","metadata":{"id":"GoEHZu2HEj5o"},"source":["## Import dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mode_install = False\n","if mode_install:\n","    !pip install tensorflow \\\n","        pillow \\\n","        matplotlib \\\n","        pandas \\\n","        scikit-learn"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1445,"status":"ok","timestamp":1622872772016,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"855u4ZVHExpn","outputId":"5366b503-7657-49d8-a6cf-eb173954c82a"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","print(tf.__version__)\n","\n","import pathlib\n","import random\n","import time\n","import PIL\n","import json"]},{"cell_type":"markdown","metadata":{"id":"KJp8MJqBOM9-"},"source":["To access **GPU** go to 'Runtime/Change runtime type' and to check if GPU is available and resources, run the following code:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":241,"status":"ok","timestamp":1622872780376,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"oroBrZJEcOPG","outputId":"3257f4b8-fd50-48c1-c0d6-fdbb8f44c3bc"},"outputs":[],"source":["# Device name\n","tf.test.gpu_device_name()\n","\n","# GPU (Tesla), memory limit (14GB)\n","from tensorflow.python.client import device_lib\n","device_lib.list_local_devices()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mFTM28s3dzPE"},"outputs":[],"source":["# Memory resources\n","!cat /proc/meminfo"]},{"cell_type":"markdown","metadata":{"id":"43yGecu0hQ2J"},"source":["## Download data"]},{"cell_type":"markdown","metadata":{"id":"WduIJ2VUhfrx"},"source":["### Mount google drive and paths"]},{"cell_type":"markdown","metadata":{"id":"o80j4-WHFpm9"},"source":["Start by mounting your google drive:   "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15961,"status":"ok","timestamp":1622872811787,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"bVqNrmTrFnkU","outputId":"98835e9a-59e9-49e9-aab2-612ee07e6adf"},"outputs":[],"source":["# Mount google drive to access files via colab\n","mode_colab = False\n","if mode_colab:\n","    from google.colab import drive\n","    drive.mount(\"/content/gdrive\")"]},{"cell_type":"markdown","metadata":{"id":"yx4SH1RDG1tc"},"source":["Specify the path of the notebook, something like /content/gdrive/MyDrive/deep-learning-for-computer-vision-with-keras/codetest/ (clik on the link to open the contents on the left pannel), and a path to save results. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["name_save = \"ensembles\"\n","\n","# Path results\n","if mode_colab:\n","      save_dir = \"/content/gdrive/MyDrive/Colab_Notebooks/Results/ensembles\"\n","      data_dir = '/content/gdrive/MyDrive/Colab_Notebooks/Data'\n","else:\n","      save_dir = \"../../Results/ensembles\"\n","      data_dir = '../../Data'\n","if os.path.exists(save_dir) is False:\n","      os.mkdir(save_dir)\n","      print(f\"Directory: {save_dir} created.\")\n","if os.path.exists(data_dir) is False:\n","      os.mkdir(data_dir)\n","      print(f\"Directory: {data_dir} created.\")"]},{"cell_type":"markdown","metadata":{"id":"ht2znQiXE1aU"},"source":["### Download data"]},{"cell_type":"markdown","metadata":{"id":"groP5H2yJwQz"},"source":["Download the data subset automatically into your drive. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"102-nO6IFvpH"},"outputs":[],"source":["dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n","data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True, cache_dir=data_dir)\n","print(f\"Data downloaded to {os.path.abspath(data_dir)}\")\n","!rm \"{data_dir}/../flower_photos.tar.gz\""]},{"cell_type":"markdown","metadata":{"id":"BlUOEBt9E7Ck"},"source":["## Define parameters and general functions"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":203,"status":"ok","timestamp":1622872869113,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"6TrtAduSFAD0"},"outputs":[],"source":["# Image sizes\n","batch_size = 32\n","img_height = 180\n","img_width = 180\n","\n","learning_rate = 1e-4"]},{"cell_type":"markdown","metadata":{"id":"e0NM7vVnhuVB"},"source":["## Data pipeline"]},{"cell_type":"markdown","metadata":{"id":"DoKQLR9MFOi-"},"source":["### Create dataset"]},{"cell_type":"markdown","metadata":{"id":"5L2tRTMZFaPf"},"source":["We use the tensorflow data API to automatize the data pipeline, chaining transformations (preprocessing and data augmentation), shuffling data. \n","\n","Next, we create dataset using 'image_dataset_from_directory' to get similar labeled dataset objects to specified folders. Split data into train, validation and test. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":658,"status":"ok","timestamp":1622872881021,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"OoOYqHnOFV7_","outputId":"d54a8d05-ed58-46cf-aae1-ede1494b500f"},"outputs":[],"source":["# Create data set \n","# Split in training and validation\n","train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","  data_dir,\n","  validation_split=0.2,\n","  subset=\"training\",\n","  seed=123,\n","  image_size=(img_height, img_width),\n","  batch_size=batch_size,\n","  shuffle=True)\n","\n","val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","  data_dir,\n","  validation_split=0.2,\n","  subset=\"validation\",\n","  seed=123,\n","  image_size=(img_height, img_width),\n","  batch_size=batch_size,\n","  shuffle=True)\n","\n","# Class names\n","class_names = train_ds.class_names\n","num_classes = len(class_names)\n","print('Classes names: ')\n","print(class_names)\n","\n","# Split Validation and Test\n","val_batch_size = val_ds.cardinality().numpy()\n","test_ds = val_ds.take(int(0.5*val_batch_size))\n","val_ds = val_ds.skip(int(0.5*val_batch_size))"]},{"cell_type":"markdown","metadata":{"id":"CLqO7PfUH75Y"},"source":["An efficient pipeline can be obtained using 'cache' which keeps the data in RAM memory after the first epoch and 'prefetch' which allows to prepare data for next batch while the model is being trained for the current batch on the GPU. Data is shuffled at each iteration for training data."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1622872881022,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"FFJcXyCNH6vu"},"outputs":[],"source":["AUTOTUNE = tf.data.AUTOTUNE\n","\n","# shuffle after cache\n","train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n","val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"]},{"cell_type":"markdown","metadata":{"id":"lWkmkafBF2FB"},"source":["### Data visualization "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":591},"executionInfo":{"elapsed":5483,"status":"ok","timestamp":1622872898055,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"fsxxN3PoF6Ie","outputId":"69e80fe2-fab2-41a2-eb60-11fa4906fe56"},"outputs":[],"source":["# Display several images\n","fig = plt.figure(figsize=(10, 10))\n","for images, labels in train_ds.take(1):\n","  for i in range(9):\n","    ax = plt.subplot(3, 3, i + 1)\n","    plt.imshow(images[i].numpy().astype(\"uint8\"))\n","    plt.title(class_names[labels[i]])\n","    plt.axis(\"off\")\n","    fig.savefig(os.path.join(save_dir, f\"{name_save}_grid.png\"), bbox_inches='tight', dpi=300)"]},{"cell_type":"markdown","metadata":{},"source":["Display an image per class"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Display an image per class\n","fig, axs = plt.subplots(1, num_classes, figsize=(15, 5))\n","for i, class_name in enumerate(class_names):\n","    # Get an image from the class\n","    image_path = os.path.join(data_dir, class_name, os.listdir(os.path.join(data_dir, class_name))[0])\n","    image = PIL.Image.open(image_path)\n","    \n","    # Display the image\n","    axs[i].imshow(image)\n","    axs[i].set_title(class_name)\n","    axs[i].axis('off')\n","\n","plt.show()\n","fig.savefig(os.path.join(save_dir, f\"{name_save}_classes.png\"), bbox_inches='tight', dpi=300)\n"]},{"cell_type":"markdown","metadata":{"id":"JbS_R3gtHdIf"},"source":["Images contain several objects and different background, which may harden the classification task."]},{"cell_type":"markdown","metadata":{"id":"OLAdRKnwG7GF"},"source":["Number of data per class:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":281},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1622872898056,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"stcweBYtG5sF","outputId":"5affcdf1-46e5-4699-ad22-e2bd8d6fb174"},"outputs":[],"source":["# Number of samples per class\n","data_dir = pathlib.Path(data_dir)\n","\n","class_counts = []\n","for class_name in class_names:\n","    class_count = len(list(data_dir.glob(class_name+'/*.jpg')))\n","    class_counts.append(class_count)\n","\n","fig = plt.figure(figsize=(4,4))\n","plt.barh(class_names, class_counts)\n","plt.title('Number per class')\n","plt.show()\n","fig.savefig(os.path.join(save_dir, f\"{name_save}_classes_counts.png\"), bbox_inches='tight', dpi=300)"]},{"cell_type":"markdown","metadata":{"id":"3I-ngUdIJGl6"},"source":["### Data augmentation"]},{"cell_type":"markdown","metadata":{"id":"uU1Y_Ts2glJM"},"source":["Data augmentation is performed using random flip,  rotation and zooming. Many more operataions can be used, such as random contrast, brightness, hue, saturation, etc. See [tf.image](https://www.tensorflow.org/api_docs/python/tf/image/) for more details."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":411},"executionInfo":{"elapsed":1311,"status":"ok","timestamp":1622872906471,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"E8tpOfM2JJvp","outputId":"ea227ca1-fc40-480f-b51c-3c149d5acc71"},"outputs":[],"source":["# Data augmentation\n","data_augmentation = keras.Sequential(\n","  [\n","    layers.RandomFlip(\"horizontal\", \n","                                                 input_shape=(img_height, \n","                                                              img_width,\n","                                                              3)),\n","    layers.RandomRotation(0.1),\n","    layers.RandomZoom(0.1),\n","  ]\n",")\n","\n","# Data augmentation example\n","fig = plt.figure(figsize=(7, 7))\n","for images, _ in train_ds.take(1):\n","  for i in range(9):\n","    augmented_images = data_augmentation(images)\n","    ax = plt.subplot(3, 3, i + 1)\n","    plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n","    plt.axis(\"off\")\n","fig.savefig(os.path.join(save_dir, f\"{name_save}_augmentation.png\"), bbox_inches='tight', dpi=300)"]},{"cell_type":"markdown","metadata":{"id":"YQFibRaAxo8M"},"source":["Callback for early stopping:"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":204,"status":"ok","timestamp":1622873743396,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"oEG6bffRMutC"},"outputs":[],"source":["# Early stopping based on a given metric\n","earlystop_cb = keras.callbacks.EarlyStopping(\n","    patience=10, \n","    monitor='val_accuracy',\n","    restore_best_weights=True)\n","\n","callbacks = [earlystop_cb]\n"]},{"cell_type":"markdown","metadata":{"id":"FpUQ8SZCyPO_"},"source":["Loading function to load test set for model evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1370,"status":"ok","timestamp":1622873747561,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"RcsIWRTCSw-F"},"outputs":[],"source":["# Load test data from test dataset. \n","def get_imgs_from_dataset(ds_test, ds_test_size):\n","    # Take images from data set ds_test: (data_test, data_test_noisy) \n","    data_test = []\n","    label_test = []\n","    count = 0\n","    for img, label in ds_test.take(ds_test_size):\n","        data_test_this = img.numpy()  \n","        label_test_this = label.numpy() \n","        if count == 0:\n","            data_test = data_test_this\n","            label_test = label_test_this\n","            count = 1\n","        else:            \n","            data_test = np.append(data_test, data_test_this, axis=0)\n","            label_test = np.append(label_test, label_test_this, axis=0)\n","    return data_test, label_test\n","    \n","# Test data\n","data_test, label_test = get_imgs_from_dataset(val_ds, len(val_ds)-1)"]},{"cell_type":"markdown","metadata":{"id":"uVvh9ZmfJhGo"},"source":["## Model, training and assessment: A transfer learning with fine tuning approach followed by ensemble voting"]},{"cell_type":"markdown","metadata":{"id":"h9s-9Yh0SG7-"},"source":["### Create the model"]},{"cell_type":"markdown","metadata":{"id":"i2WoPwN9JnLy"},"source":["We use transfer learning using a pretrained model that has been trained on a very large dataset (ImageNet). We try to different models: 'Xception' which provides a high top-5 accuracy (with 20 M parameters) and 'MobileNetV2' which provides great accuracy for a relatively small model size. \n","\n","We load the model but skip the 'top' layer to tailored our model to the classes in the dataset. Then, we freeze their layers to train on a small dataset. We also define the model and specify their preprocessing steps. "]},{"cell_type":"markdown","metadata":{"id":"loiGuh-QR_8g"},"source":["Instead of creating and training the model, you can load the trained model: run the code below."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_base_model(base_model_name, input_shape):\n","    # Download pretrained base model \n","    if base_model_name == 'mobilenet_v2':\n","        # mobilenet_v2: small networks\n","        # Param M: 4.24, top-1 acc: 70.9, top-5 acc:\t89.9\n","        # Imagenet ILSVRC-2012-CLS \n","        # Timing: # GTX: 7s and 3s, FT: \n","        base_model = tf.keras.applications.MobileNetV2(\n","                input_shape=input_shape,                                                   \n","                include_top=False,                                                   \n","                weights='imagenet')\n","        preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n","    elif base_model_name == 'mobilenet_v3':    \n","        # mobilenet_v3: small networks\n","        base_model = tf.keras.applications.MobileNetV3Small(\n","                input_shape=input_shape,                                                   \n","                include_top=False,                                                   \n","                weights='imagenet')\n","        preprocess_input = tf.keras.applications.mobilenet_v3.preprocess_input\n","    elif base_model_name == 'EfficientNetB0':\n","        # EfficientNetB0: 5M\n","        base_model = keras.applications.EfficientNetB0(\n","                input_shape=input_shape,                                                   \n","                include_top=False,                                                   \n","                weights='imagenet')\n","        preprocess_input = tf.keras.applications.efficientnet.preprocess_input    \n","    elif base_model_name == 'EfficientNetB4':\n","        # EfficientNetB4:19M\n","        base_model = keras.applications.EfficientNetB4(\n","                input_shape=input_shape,                                                   \n","                include_top=False,                                                   \n","                weights='imagenet')\n","        preprocess_input = tf.keras.applications.efficientnet.preprocess_input\n","    elif base_model_name == 'EfficientNetB7':\n","        # EfficientNetB7: 66M\n","        base_model = keras.applications.EfficientNetB7(\n","                input_shape=input_shape,                                                   \n","                include_top=False,                                                   \n","                weights='imagenet')\n","        preprocess_input = tf.keras.applications.efficientnet.preprocess_input\n","    elif base_model_name == 'Xception':\n","        # Xception\n","        # 20 M parameters\n","        # no smaller than 71. E.g. (150, 150, 3) \n","        # Timing: # GTX: 10s and 6s, FT: 15s and 9s\n","        base_model = keras.applications.Xception(\n","                input_shape=input_shape,                                                   \n","                include_top=False,                                                   \n","                weights='imagenet')\n","        preprocess_input = tf.keras.applications.xception.preprocess_input\n","    elif base_model_name == 'ResNet50':\n","        # ResNet50: 25 M\n","        base_model = keras.applications.ResNet50(\n","                input_shape=input_shape,                                                   \n","                include_top=False,                                                   \n","                weights='imagenet')\n","        preprocess_input = tf.keras.applications.resnet50.preprocess_input\n","    elif base_model_name == 'vgg19':\n","        # VGG19: 143 M \n","        base_model = keras.applications.VGG19(\n","                input_shape=input_shape,                                                   \n","                include_top=False,                                                   \n","                weights='imagenet')\n","        preprocess_input = tf.keras.applications.vgg19.preprocess_input\n","    elif base_model_name == 'inception_v3':\n","        # InceptionV3: 23M\n","        base_model = keras.applications.InceptionV3(\n","                input_shape=input_shape,                                                   \n","                include_top=False,                                                   \n","                weights='imagenet')\n","        preprocess_input = tf.keras.applications.inception_v3.preprocess_input\n","    \n","    # Freeze weights\n","    base_model.trainable = False\n","    return base_model, preprocess_input"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_model(inputs_shape, preprocess_input, base_model, \n","              num_classes=num_classes, dropout_rate=0.2):\n","\n","    # Average pooling layer to pass from block 6x6x1280 to vector 1x1280\n","    global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n","\n","    # Multiclass classification layer\n","    prediction_layer = tf.keras.layers.Dense(num_classes)\n","\n","    # Create the model\n","    inputs = tf.keras.Input(shape=inputs_shape)\n","    x = data_augmentation(inputs)\n","    x = preprocess_input(x)        \n","    x = base_model(x, training=False)\n","    x = global_average_layer(x)\n","    x = layers.Dropout(dropout_rate)(x)\n","    outputs = prediction_layer(x)\n","    model = tf.keras.Model(inputs, outputs)\n","\n","    model.summary()\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def freeze_layers(base_model, fine_tune_at):    \n","    # Unfreeze the base model\n","    base_model.trainable = True\n","\n","    # Number layers are in the base model\n","    print(\"Number of layers in the base model: \", len(base_model.layers))\n","\n","    # Freeze all the layers before the `fine_tune_at` layer\n","    for layer in base_model.layers[:fine_tune_at]:\n","        layer.trainable =  False\n","    return base_model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def display_training_curves(acc, val_acc, loss, val_loss, name_save):\n","  epochs_range = range(len(acc))\n","\n","  fig = plt.figure(figsize=(8, 8))\n","  plt.subplot(1, 2, 1)\n","  plt.plot(epochs_range, acc, label='Training Accuracy')\n","  plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n","  plt.legend(loc='lower right')\n","  plt.title('Training and Validation Accuracy')\n","  #\n","  plt.subplot(1, 2, 2)\n","  plt.plot(epochs_range, loss, label='Training Loss')\n","  plt.plot(epochs_range, val_loss, label='Validation Loss')\n","  plt.legend(loc='upper right')\n","  plt.title('Training and Validation Loss')\n","  plt.show()\n","  fig.savefig(name_save, bbox_inches='tight', dpi=300)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Loss, metrics, optimizer\n","acc_fn = tf.keras.metrics.Accuracy()\n","loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","metrics = ['accuracy']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"CzHyjgKvKJWc","outputId":"b0b936b7-1786-4313-9521-594a19972240"},"outputs":[],"source":["# Number of models for ensemble voting\n","IMG_SHAPE = (img_height, img_width) + (3,)\n","fine_tune_at = 70     # Fine-tune from this layer onwards\n","epochs = 100          # 100\n","fine_tune_epochs = 70 #70\n","\n","# Xception EfficientNetB4 inception_v3  ResNet50 vgg19 \n","# mobilenet_v2 mobilenet_v3 EfficientNetB0 EfficientNetB7\n","base_model_name_list = ['mobilenet_v2', 'mobilenet_v3', 'EfficientNetB0']\n","test_acc_models = []\n","models = []\n","for base_model_name in base_model_name_list:\n","  # Define pretrained base model\n","  print('*'*50)\n","  print(f\"Base model: {base_model_name}\") \n","  base_model, preprocess_input = get_base_model(base_model_name, IMG_SHAPE)\n","\n","  # Model name\n","  model = get_model(IMG_SHAPE, preprocess_input, base_model)\n","\n","  # Compile the model\n","  optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","  model.compile(optimizer=optimizer,\n","                loss=loss_fn,\n","                metrics=metrics)\n","\n","  # Train a large number of steps with early stopping criterion\n","  history = model.fit(train_ds, validation_data=val_ds, epochs=epochs, \n","                      callbacks=callbacks, verbose = 0)\n","\n","  # Display loss and accuracy\n","  acc = history.history['accuracy']\n","  val_acc = history.history['val_accuracy']\n","  loss = history.history['loss']\n","  val_loss = history.history['val_loss']\n","\n","  # FINE TUNING      \n","  base_model = freeze_layers(base_model, fine_tune_at)\n","\n","  # Recompile your model after you make any changes\n","  optimizer_ft = tf.keras.optimizers.RMSprop(learning_rate=learning_rate/10)\n","  model.compile(optimizer = optimizer_ft,  # Lower learning rate\n","                loss=loss_fn,\n","                metrics=metrics)\n","  # ---------------------------------------------------------------------\n","  # Train (continue training)\n","  total_epochs = epochs + fine_tune_epochs    \n","  history_fine = model.fit(train_ds,\n","                            epochs=total_epochs,\n","                            initial_epoch=history.epoch[-1],\n","                            validation_data=val_ds,\n","                            callbacks=callbacks,\n","                           verbose=0)\n","\n","  # Display losses\n","  acc += history_fine.history['accuracy']\n","  val_acc += history_fine.history['val_accuracy']\n","\n","  loss += history_fine.history['loss']\n","  val_loss += history_fine.history['val_loss']\n","\n","  epochs_range = range(len(acc))\n","\n","  name_save_this = os.path.join(save_dir, f\"{name_save}_{base_model_name}_loss_ep{epochs}_ft{fine_tune_epochs}.png\")\n","  display_training_curves(acc, val_acc, loss, val_loss, name_save)\n","\n","  # Predict \n","  data_pred = model.predict(data_test)\n","  data_pred_class = np.argmax(data_pred, axis=1)\n","\n","  test_acc = acc_fn(data_pred_class, label_test)\n","  print('Accuracy for %s with fine tuning is %.2f' % (base_model_name, test_acc))\n","  test_acc_models.append(test_acc)\n","\n","  # Save the model in keras format (default).\n","  name_save = 'flower_photos_' + base_model_name + '_TF' + str(history.epoch[-1]) +'it' + '_FTat' + str(fine_tune_at) + '_it' + str(history_fine.epoch[-1]) + '_Ens' + str(i_ensemble) \n","  model.save(os.path.join(save_dir, name_save + \".keras\"))\n","  print(f\"Model saved in {os.path.abspath(os.path.join(save_dir, name_save))}\")\n","  models.append(model)  \n","\n","  # Save history\n","  with open(os.path.join(save_dir, name_save + '.json'), 'w') as file:\n","    json.dump(history_fine.history, file)"]},{"cell_type":"markdown","metadata":{"id":"T_Vztz2sz13t"},"source":["### Ensemble assessment "]},{"cell_type":"markdown","metadata":{},"source":["We combined the individually trained models by majority voting."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def ensemble_voting(data_pred_list):\n","    # Voting\n","    data_pred_ensemble = np.mean(data_pred_list, axis=0)\n","    data_pred_class_ensemble = np.argmax(data_pred_ensemble, axis=1)\n","    return data_pred_class_ensemble"]},{"cell_type":"markdown","metadata":{},"source":["Another option is to create a combined ensembled model and train it, based on this [blog article](https://blog.paperspace.com/ensembling-neural-network-models/). "]},{"cell_type":"markdown","metadata":{},"source":["### Model assessment"]},{"cell_type":"markdown","metadata":{},"source":["Alternatively, we can load the data, predict and compute the desired metric."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Predict for all models\n","data_pred_list = []\n","test_acc_models = []\n","for model in models:\n","    data_pred = model.predict(data_test)\n","    data_pred_list.append(data_pred)\n","    data_pred_class = np.argmax(data_pred, axis=1)\n","    test_acc = acc_fn(data_pred_class, label_test)\n","    test_acc_models.append(test_acc)\n","\n","test_acc_models = [acc.numpy() for acc in test_acc_models]\n","print('Accuracy for each model is: ', test_acc_models)\n","\n","# Ensemble voting\n","data_pred_class_ensemble = ensemble_voting(data_pred_list)\n","test_acc_ensemble = acc_fn(data_pred_class_ensemble, label_test)\n","print('Accuracy for ensemble is %.5f' % test_acc_ensemble)"]},{"cell_type":"markdown","metadata":{},"source":["Ensemble voting leads to a better performance than individual models and generally slighltly better than the best individual model."]},{"cell_type":"markdown","metadata":{},"source":["## References"]},{"cell_type":"markdown","metadata":{},"source":["- M.A. Ganaie et al. (2022). Ensemble Learning: A Review. Engineering Applications of Artificial Intelligence, 115: 105151, 2022. "]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPj+PbjPsaxuOemUQ6Z8/Jv","collapsed_sections":["IwoYfJ9ntIhm","am_XRmBBfwWb"],"name":"image_classification_using_transfer_learning_ensembles.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
