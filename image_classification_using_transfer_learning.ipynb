{"cells":[{"cell_type":"markdown","metadata":{},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jabascal/deep-learning-for-computer-vision-with-keras/blob/main/image_classification_using_transfer_learning.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"3Gl9vcdgAois"},"source":["# Image classification using transfer learning"]},{"cell_type":"markdown","metadata":{"id":"RzBtnMxxAuRR"},"source":["The objectives of this notebook are to create a classifier that leads to the largest accuracy on the given dataset using a transfer learning and fine-tuning approach. \n","\n","For *transfer learning*, we use the pretrained weights of an architecture pretrained on a large-scale dataset (ImageNet), excluding the top classification layer. Then, we freeze the rest of layers and add a classification layer adapted to the number of classes in our specific dataset. Finally, we train the new model, which has a very low number of weights (few thousands) compared to  the base model (with several millions), on a specific and smaller dataset. \n","\n","For *fine tuning*, we unfreeze few or all of the top layers of the base model and then tune the weights to obtain higher accuracy. "]},{"cell_type":"markdown","metadata":{"id":"GoEHZu2HEj5o"},"source":["## Import dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mode_install = False\n","if mode_install:\n","    !pip install tensorflow \\\n","        pillow \\\n","        matplotlib \\\n","        pandas \\\n","        scikit-learn"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2183,"status":"ok","timestamp":1623156681592,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"855u4ZVHExpn","outputId":"6248d991-7747-4eaf-e68b-71997c2b3496"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","print(f\"TF version {tf.__version__}\")\n","\n","import pathlib\n","import random\n","import time\n","import PIL"]},{"cell_type":"markdown","metadata":{"id":"KJp8MJqBOM9-"},"source":["To access **GPU** go to 'Runtime/Change runtime type' and to check if GPU is available and resources, run the following code:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oroBrZJEcOPG"},"outputs":[],"source":["# Device name\n","tf.test.gpu_device_name()\n","\n","# GPU (Tesla), memory limit (14GB)\n","from tensorflow.python.client import device_lib\n","device_lib.list_local_devices()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mFTM28s3dzPE"},"outputs":[],"source":["# Memory resources\n","!cat /proc/meminfo"]},{"cell_type":"markdown","metadata":{"id":"43yGecu0hQ2J"},"source":["## Download data"]},{"cell_type":"markdown","metadata":{"id":"WduIJ2VUhfrx"},"source":["### Mount google drive and paths"]},{"cell_type":"markdown","metadata":{"id":"o80j4-WHFpm9"},"source":["Start by mounting your google drive:   "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17019,"status":"ok","timestamp":1623156700031,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"bVqNrmTrFnkU","outputId":"a33dbfce-ffdf-4907-d456-78b180355b54"},"outputs":[],"source":["# Mount google drive to access files via colab\n","mode_colab = True\n","if mode_colab:\n","    from google.colab import drive\n","    drive.mount(\"/content/gdrive\")"]},{"cell_type":"markdown","metadata":{"id":"yx4SH1RDG1tc"},"source":["Specify the path for saving data and results."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":353,"status":"ok","timestamp":1623156711939,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"ML-M5fwxG0Ul"},"outputs":[],"source":["# Path notebook\n","#notebook_dir = \"/content/gdrive/MyDrive/Colab_Notebooks/deep-learning-for-computer-vision-with-keras/\"\n","#os.chdir(notebook_dir)\n","\n","name_save = \"flower\"\n","\n","# Path results\n","if mode_colab:\n","      save_dir = \"/content/gdrive/MyDrive/Colab_Notebooks/Results/flower_photos\"\n","      data_dir = '/content/gdrive/MyDrive/Colab_Notebooks/Data'\n","else:\n","      save_dir = \"../../Results/flower_photos\"\n","      data_dir = '../../Data'\n","if os.path.exists(save_dir) is False:\n","      os.mkdir(save_dir)\n","      print(f\"Directory: {save_dir} created.\")\n","if os.path.exists(data_dir) is False:\n","      os.mkdir(data_dir)\n","      print(f\"Directory: {data_dir} created.\")"]},{"cell_type":"markdown","metadata":{"id":"ht2znQiXE1aU"},"source":["### Download data"]},{"cell_type":"markdown","metadata":{"id":"groP5H2yJwQz"},"source":["Download the data subset automatically into your drive to the directory *data_dir*: '/content/gdrive/MyDrive/Colab_Notebooks/Data/'"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n","data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True, cache_dir=data_dir)\n","print(f\"Data downloaded to {os.path.abspath(data_dir)}\")\n","!rm \"{data_dir}/../flower_photos.tar.gz\"\n","#os.chdir(data_new_dir)\n","#!wget -P path_notebook -O flower_photos.tgz https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\n","#!tar -xzf os.path.join(data_dir, 'flower_photos.tgz')\n","#!rm flower_photos.tgz"]},{"cell_type":"markdown","metadata":{"id":"BlUOEBt9E7Ck"},"source":["## Define parameters and general functions"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1623157767283,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"6TrtAduSFAD0"},"outputs":[],"source":["# Image sizes (depends on the model)\n","img_height = 180 \n","img_width = 180\n","\n","batch_size = 256\n","learning_rate = 1e-4"]},{"cell_type":"markdown","metadata":{"id":"e0NM7vVnhuVB"},"source":["## Data pipeline"]},{"cell_type":"markdown","metadata":{"id":"DoKQLR9MFOi-"},"source":["### Create dataset"]},{"cell_type":"markdown","metadata":{"id":"5L2tRTMZFaPf"},"source":["We use the tensorflow data API to automatize the data pipeline, chaining transformations (preprocessing and data augmentation), shuffling data. \n","\n","Next, we create dataset using 'image_dataset_from_directory' to get similar labeled dataset objects to specified folders. Split data into train, validation and test. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6083,"status":"ok","timestamp":1623157819405,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"OoOYqHnOFV7_","outputId":"196babf9-4872-4202-8a72-e738ee0a8c86"},"outputs":[],"source":["# Create data set \n","# Split in training and validation\n","train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","  data_dir,\n","  validation_split=0.2,\n","  subset=\"training\",\n","  seed=123,\n","  image_size=(img_height, img_width),\n","  batch_size=batch_size,\n","  shuffle=True)\n","\n","val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","  data_dir,\n","  validation_split=0.2,\n","  subset=\"validation\",\n","  seed=123,\n","  image_size=(img_height, img_width),\n","  batch_size=batch_size,\n","  shuffle=True)\n","\n","# Class names\n","class_names = train_ds.class_names\n","num_classes = len(class_names)\n","print(f'Classes names: {class_names}')\n","\n","# Split Validation and Test\n","val_batch_size = val_ds.cardinality().numpy()\n","test_ds = val_ds.take(int(0.5*val_batch_size))\n","val_ds = val_ds.skip(int(0.5*val_batch_size))"]},{"cell_type":"markdown","metadata":{"id":"CLqO7PfUH75Y"},"source":["An efficient pipeline can be obtained using 'cache' which keeps the data in RAM memory after the first epoch and 'prefetch' which allows to prepare data for next batch while the model is being trained for the current batch on the GPU. Data is shuffled at each iteration for training data."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":334,"status":"ok","timestamp":1623157824700,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"FFJcXyCNH6vu"},"outputs":[],"source":["AUTOTUNE = tf.data.AUTOTUNE\n","\n","# shuffle after cache\n","train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n","val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"]},{"cell_type":"markdown","metadata":{"id":"lWkmkafBF2FB"},"source":["### Data visualization "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":591},"executionInfo":{"elapsed":13910,"status":"ok","timestamp":1623157848699,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"fsxxN3PoF6Ie","outputId":"d8f92c94-c988-42b2-96f0-506ae9079422"},"outputs":[],"source":["# Display several images\n","fig = plt.figure(figsize=(10, 10))\n","for images, labels in train_ds.take(1):\n","  for i in range(9):\n","    ax = plt.subplot(3, 3, i + 1)\n","    plt.imshow(images[i].numpy().astype(\"uint8\"))\n","    plt.title(class_names[labels[i]])\n","    plt.axis(\"off\")\n","    fig.savefig(os.path.join(save_dir, f\"{name_save}_grid.png\"), bbox_inches='tight', dpi=300)"]},{"cell_type":"markdown","metadata":{},"source":["Display an image per class"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Display an image per class\n","fig, axs = plt.subplots(1, num_classes, figsize=(15, 5))\n","for i, class_name in enumerate(class_names):\n","    # Get an image from the class\n","    image_path = os.path.join(data_dir, class_name, os.listdir(os.path.join(data_dir, class_name))[0])\n","    image = PIL.Image.open(image_path)\n","    \n","    # Display the image\n","    axs[i].imshow(image)\n","    axs[i].set_title(class_name)\n","    axs[i].axis('off')\n","\n","plt.show()\n","fig.savefig(os.path.join(save_dir, f\"{name_save}_classes.png\"), bbox_inches='tight', dpi=300)\n"]},{"cell_type":"markdown","metadata":{"id":"JbS_R3gtHdIf"},"source":["Images contain several objects and different background, which may harden the classification task."]},{"cell_type":"markdown","metadata":{"id":"OLAdRKnwG7GF"},"source":["Number of data per class:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":281},"executionInfo":{"elapsed":346,"status":"ok","timestamp":1623157874365,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"stcweBYtG5sF","outputId":"d04a165c-1dc2-41a1-9dc4-c5099c005ef5"},"outputs":[],"source":["# Number of samples per class\n","data_dir = pathlib.Path(data_dir)\n","\n","class_counts = []\n","for class_name in class_names:\n","    class_count = len(list(data_dir.glob(class_name+'/*.jpg')))\n","    class_counts.append(class_count)\n","\n","fig = plt.figure(figsize=(4,4))\n","plt.barh(class_names, class_counts)\n","plt.title('Number per class')\n","plt.show()\n","fig.savefig(os.path.join(save_dir, f\"{name_save}_classes_counts.png\"), bbox_inches='tight', dpi=300)"]},{"cell_type":"markdown","metadata":{"id":"3I-ngUdIJGl6"},"source":["### Data augmentation"]},{"cell_type":"markdown","metadata":{"id":"uU1Y_Ts2glJM"},"source":["Data augmentation is performed using random flip,  rotation and zooming. Many more operataions can be used, such as random contrast, brightness, hue, saturation, etc. See [tf.image](https://www.tensorflow.org/api_docs/python/tf/image/) for more details."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":411},"executionInfo":{"elapsed":2426,"status":"ok","timestamp":1623157906721,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"E8tpOfM2JJvp","outputId":"48bbf8a1-fa3b-4e59-91e2-68d49f80d77e"},"outputs":[],"source":["# Data augmentation\n","data_augmentation = keras.Sequential(\n","  [\n","    layers.RandomFlip(\"horizontal\", input_shape=(img_height, img_width, 3)),\n","    layers.RandomRotation(0.1),\n","    layers.RandomZoom(0.1),\n","  ], name=\"data_augmentation\"\n",")\n","\n","# Data augmentation example\n","fig = plt.figure(figsize=(7, 7))\n","for images, _ in train_ds.take(1):\n","  for i in range(9):\n","    augmented_images = data_augmentation(images)\n","    ax = plt.subplot(3, 3, i + 1)\n","    plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n","    plt.axis(\"off\")\n","fig.savefig(os.path.join(save_dir, f\"{name_save}_augmentation.png\"), bbox_inches='tight', dpi=300)"]},{"cell_type":"markdown","metadata":{"id":"uVvh9ZmfJhGo"},"source":["## Model, training and assessment: A transfer learning approach"]},{"cell_type":"markdown","metadata":{"id":"h9s-9Yh0SG7-"},"source":["### Create the model"]},{"cell_type":"markdown","metadata":{"id":"i2WoPwN9JnLy"},"source":["We use transfer learning using a pretrained model that has been trained on a very large dataset (ImageNet). We can try different models: 'Xception' which provides a high top-5 accuracy (with 20 M parameters) and 'MobileNetV2' which provides great accuracy for a relatively small model size (4 M parameters). \n","\n","We load the model but skip the 'top' layer to tailored our model to the classes in the dataset. Then, we freeze their layers to train on a small dataset. We also define the model and specify their preprocessing steps. "]},{"cell_type":"markdown","metadata":{"id":"loiGuh-QR_8g"},"source":["Instead of creating and training the model, you can load the trained model: run the code below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DpyKJZSSSL7A"},"outputs":[],"source":["model_load_model = False\n","if model_load_model is True: \n","  model = keras.models.load_model(os.path.join(save_dir,'flower_photos_Xception_TF62it.h5'))\n","  epochs = 62"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3150,"status":"ok","timestamp":1623157933864,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"CzHyjgKvKJWc","outputId":"8ef7db99-575f-474e-8ef7-59f815cae9d1"},"outputs":[],"source":["IMG_SHAPE = (img_height, img_width) + (3,)\n","\n","# Download pretrained base model\n","base_model_name = 'mobilenet_v2' # mobilenet_v2, Xception, vgg16\n","if base_model_name == 'mobilenet_v2':\n","    # mobilenet_v2: small networks\n","    # Param M: 4.24, top-1 acc: 70.9, top-5 acc:\t89.9\n","    # Imagenet ILSVRC-2012-CLS \n","    # Timing: # GTX: 7s and 3s, FT: \n","    base_model = tf.keras.applications.MobileNetV2(\n","            input_shape=IMG_SHAPE,                                                   \n","            include_top=False,                                                   \n","            weights='imagenet')\n","    preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input    \n","elif base_model_name == 'Xception':\n","    # Xception\n","    # 20 M parameters\n","    # no smaller than 71. E.g. (150, 150, 3) \n","    # Timing: # GTX: 10s and 6s, FT: 15s and 9s\n","    base_model = keras.applications.Xception(\n","            input_shape=IMG_SHAPE,                                                   \n","            include_top=False,                                                   \n","            weights='imagenet')\n","    preprocess_input = tf.keras.layers.experimental.preprocessing.Rescaling(1./127.5, offset= -1)\n","\n","# Freeze weights\n","base_model.trainable = False\n","\n","base_model.summary()"]},{"cell_type":"markdown","metadata":{"id":"0KAbxmjXLPec"},"source":["To create the model, we define the top layers, a pooling layer and the classification layer, specifying the number of classes, and then concatenate all layers. Data augmentation and pre-processing are included as bottom layers in the model. \n","\n","We also add a dropout layer to avoid overfitting. It drops a fraction of the input units and has a similar effect as L2 regularization."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1101,"status":"ok","timestamp":1623157938832,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"huuzuirqKj_s","outputId":"330096ba-01d7-4e61-e862-a54802a0c865"},"outputs":[],"source":["def get_model(inputs_shape, num_classes=num_classes, dropout_rate=0.2):\n","\n","    # Average pooling layer to pass from block 6x6x1280 to vector 1x1280\n","    global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n","\n","    # Multiclass classification layer\n","    prediction_layer = tf.keras.layers.Dense(num_classes)\n","\n","    # Create the model\n","    inputs = tf.keras.Input(shape=inputs_shape)\n","    x = data_augmentation(inputs)\n","    x = preprocess_input(x)        \n","    x = base_model(x, training=False)\n","    x = global_average_layer(x)\n","    x = layers.Dropout(dropout_rate)(x)\n","    outputs = prediction_layer(x)\n","    model = tf.keras.Model(inputs, outputs)\n","\n","    model.summary()\n","    return model\n","\n","model = get_model(inputs_shape=(img_height, img_width, 3), \n","                  num_classes=num_classes)"]},{"cell_type":"markdown","metadata":{"id":"MNb1EaA2iMWI"},"source":["The pretrained Xception model has 21 M parameters, which have been pretrained on 14 M images. On the transfer learning step we aim to fit instead only 10 thousand parameters using 4 thousand images. "]},{"cell_type":"markdown","metadata":{"id":"UAWM2v49MRMb"},"source":["We define callbacks to for tensorboard (weights analysis and profiler) and for early stopping. For other callbacks, see [tf.keras.callbacks](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":328,"status":"ok","timestamp":1623157942890,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"oEG6bffRMutC","outputId":"d070758b-dad7-4c5d-bc0e-5916980189e1"},"outputs":[],"source":["# LOG DIRECTORY FOR TENSORBOARD\n","def get_run_logdir(path_log):\n","    now = time.strftime(\"run_%Y-%m-%d-%H-%M-%S\")\n","    log_dir = os.path.join(save_dir, \"runs\", now)\n","    print(f\"Log directory: {log_dir}\")\n","    return log_dir\n","\n","# CALLBACKS \n","# Tensorboard\n","# tensorboard --logdir=log_dir\n","log_dir = get_run_logdir(save_dir)    \n","\n","# Enable visualizations for Tensorboard\n","tensorboard_cb = keras.callbacks.TensorBoard(\n","    log_dir,\n","    histogram_freq=10, # Histograms of weights\n","    write_images=True, # Images for weights\n","    profile_batch = '2,3') # Profiling: improve performance https://www.tensorflow.org/guide/profiler\n","\n","# Early stopping based on a given metric\n","earlystop_cb = keras.callbacks.EarlyStopping(\n","    patience=20, \n","    monitor='val_accuracy',\n","    restore_best_weights=True)\n","\n","# Model checkpoint\n","tf_vs = str(tf.__version__).replace(\".\", \"-\")\n","checkpoint_path = os.path.join(save_dir, \"training\",\n","                               f\"{name_save}_{base_model_name}_TF{tf_vs}\"+\"cp-{epoch:04d}.ckpt\")\n","checkpoint_cb = keras.callbacks.ModelCheckpoint(\n","    filepath = checkpoint_path,\n","    save_weights_only=True,\n","    save_best_only=True,\n","    verbose=1,\n","    save_freq=10)\n","\n","callbacks = [tensorboard_cb, earlystop_cb]"]},{"cell_type":"markdown","metadata":{},"source":["Checkpoints save only weights, in binary format on one or more shards, not the model. Weights are save in `.ckpt` format. To save in  HDF5 format add the extension `.h5` to the file name. \n","\n","To load the model, you need to define the model and then load the weights.\n","```\n","    # Save (manually) the weights only \n","    model.save_weights(checkpoint_path)\n","\n","    # Create a new model instance\n","    model = create_model()\n","\n","    # Load the weights\n","    model.load_weights(checkpoint_path)\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"2O7wHUK6Rxk1"},"source":["### Train the model "]},{"cell_type":"markdown","metadata":{"id":"is7y9QPARzxV"},"source":["We compile the model, specifying the loss and learning rate, and train. We aim to to obtain a high accuracy as we are fine tuning lower layers in a later fine tuning step. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":730147,"status":"ok","timestamp":1622833879356,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"hzcXpLV0MIsM","outputId":"ac2db3fa-4bde-429d-95d6-82d71ed7d62f"},"outputs":[],"source":["# Compile the model\n","model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","\n","# Train a large number of steps with early stopping criterion\n","epochs=70\n","history = model.fit(\n","  train_ds,\n","  validation_data=val_ds,\n","  epochs=epochs,\n","  callbacks=callbacks\n",")\n"]},{"cell_type":"markdown","metadata":{"id":"s31m_Pt0OfyW"},"source":["Training took $10$ s per epoch (took $6$ s/step on GTX 1080 Ti) and used 2.5 GB. \n","\n","Below, we display loss and accuracy. We see that the validation loss is starting to flatten out, with a final accuracy of $89$ %. We could take more steps but we leave it for the fine tuning step. \n","\n","We remark that even though we use a early stopping criterion, the model did not stop (it run only $63$ epochs on my local computer); more iterations could be set. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":516},"executionInfo":{"elapsed":684,"status":"ok","timestamp":1622834037513,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"3crAzeoCQCSt","outputId":"11589a8f-a97d-4879-cb61-d3fb4eb01e4f"},"outputs":[],"source":["# Display loss and accuracy\n","acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","# Visualize metrics\n","epochs_range = range(len(acc))\n","\n","fig = plt.figure(figsize=(8, 8))\n","plt.subplot(1, 2, 1)\n","plt.plot(epochs_range, acc, label='Training Accuracy')\n","plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n","plt.legend(loc='lower right')\n","plt.title('Training and Validation Accuracy')\n","fig.savefig(os.path.join(save_dir, f\"{name_save}_accuracy.png\"), bbox_inches='tight', dpi=300)\n","#\n","plt.subplot(1, 2, 2)\n","plt.plot(epochs_range, loss, label='Training Loss')\n","plt.plot(epochs_range, val_loss, label='Validation Loss')\n","plt.legend(loc='upper right')\n","plt.title('Training and Validation Loss')\n","fig.savefig(os.path.join(save_dir, f\"{name_save}_loss.png\"), bbox_inches='tight', dpi=300)\n","plt.show()\n","\n","print('Accuracy on validation set at the final step %.2f' %(acc[-1]))"]},{"cell_type":"markdown","metadata":{"id":"5XasaUGenNpV"},"source":["Save the entired trained model we use `tf.keras.Model.save` to save a model's architecture, weights and training configuration in a single file. Threre are several formats to save the model: `.keras` (Keras v3) and two legacy formats `.h5` and `SavedModel`, specifying `save_format`. These can be load in web browser or convert them to TFLite format for mobile devices."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A01SDE5unNpe"},"outputs":[],"source":["# Save the model \n","model_path = os.path.join(save_dir, f'{name_save}_{base_model_name}_TF{tf_vs}_ep{str(history.epoch[-1])}.keras')\n","model.save(model_path)\n","print(f\"Model saved in {os.path.join(save_dir, name_save)}\")\n","print(os.listdir(save_dir))"]},{"cell_type":"markdown","metadata":{"id":"2fUI5DX6i8lF"},"source":["Save history."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KZxCMFYTimYS"},"outputs":[],"source":["# Save history\n","import json\n","with open(os.path.join(save_dir, f'{name_save}_hist.json'), 'w') as file:\n","  json.dump(history.history, file)"]},{"cell_type":"markdown","metadata":{},"source":["We asses the model on the test dataset using `Model.evaluate`, which returns the loss and metrics values."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["loss_assess, accuracy_assess = model.evaluate(test_ds)\n","print('Test accuracy :', accuracy_assess)"]},{"cell_type":"markdown","metadata":{"id":"kzr02CZNT_N9"},"source":["## Model, training and assessment: Further fine tuning step"]},{"cell_type":"markdown","metadata":{"id":"-8t7lXZFUJDV"},"source":["To improve further the accuracy we fine tune lower layers. Once the model has converged on the new data, we unfreeze part of the base model and retrain the whole model end-to-end with a very low learning rate. The 'Xception' model has $132$ layers. We fine tune all layers after layer $70$, which leads to $10$ M from the $20$ M parameters in the full model. This needs $17$ s per epoch."]},{"cell_type":"markdown","metadata":{"id":"Yy7ViL1w-LUs"},"source":["Instead of creating and training the model, you can load the trained model: run the code below."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6481,"status":"ok","timestamp":1623158010427,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"kKoTTYto-LUz"},"outputs":[],"source":["model_load_model = False\n","if model_load_model is True: \n","  model = keras.models.load_model(os.path.join(save_dir,'flower_photos_Xception_TF62it_FTat70_it105.h5'))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":235,"status":"ok","timestamp":1622834189320,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"G5M05h1fUIGD","outputId":"43b2418b-817b-4761-a0ac-a9a774fdefba"},"outputs":[],"source":["# FINE TUNING\n","# Unfreeze the base model\n","base_model.trainable = True\n","\n","# Number layers are in the base model\n","print(\"Number of layers in the base model: \", len(base_model.layers))\n","\n","# Fine-tune from this layer onwards\n","fine_tune_at = 70\n","\n","# Freeze all the layers before the `fine_tune_at` layer\n","for layer in base_model.layers[:fine_tune_at]:\n","    layer.trainable =  False\n","    \n","# Recompile your model after you make any changes\n","model.compile(optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate/10),  # Lower learning rate\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"-kted7dVXtD8"},"source":["Train the new model."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":814922,"status":"ok","timestamp":1622835021189,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"1Lr9znEfVEnX","outputId":"665f6098-edf7-4383-d1fe-e78e802d40a1"},"outputs":[],"source":["# Train (continue training)\n","fine_tune_epochs = 70\n","total_epochs = epochs + fine_tune_epochs    \n","history_fine = model.fit(train_ds,\n","                          epochs=total_epochs,\n","                          initial_epoch=history.epoch[-1],\n","                          validation_data=val_ds,\n","                          callbacks=callbacks)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":499},"executionInfo":{"elapsed":766,"status":"ok","timestamp":1622835080993,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"U1Nh2jZIVnpZ","outputId":"2fdfaff7-2912-436b-a56d-c22bb65edb64"},"outputs":[],"source":["# Display losses\n","acc += history_fine.history['accuracy']\n","val_acc += history_fine.history['val_accuracy']\n","\n","loss += history_fine.history['loss']\n","val_loss += history_fine.history['val_loss']\n","\n","epochs_range = range(len(acc))\n","fig = plt.figure(figsize=(8, 8))\n","plt.subplot(1, 2, 1)\n","plt.plot(epochs_range, acc, label='Training Accuracy')\n","plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n","plt.ylim([0.5, 1])\n","plt.plot([epochs-1,epochs-1],\n","          plt.ylim(), label='Start Fine Tuning')\n","plt.legend(loc='lower right')\n","plt.title('Training and Validation Accuracy')\n","#\n","plt.subplot(1, 2, 2)\n","plt.plot(epochs_range, loss, label='Training Loss')\n","plt.plot(epochs_range, val_loss, label='Validation Loss')\n","plt.ylim([0, 1.0])\n","plt.plot([epochs-1,epochs-1],\n","          plt.ylim(), label='Start Fine Tuning')\n","plt.legend(loc='upper right')\n","plt.title('Training and Validation Loss')\n","plt.show()\n","fig.savefig(os.path.join(save_dir, f\"{name_save}_loss_fine.png\"), bbox_inches='tight', dpi=300)"]},{"cell_type":"markdown","metadata":{"id":"v69w4pcQWAja"},"source":["Predict and get accuracy on test set of $95$ %! There is  a clear jump in accuracy after fine tuning. We remark that the metric used for early stoping is the 'accuracy' so although the validation loss is decreasing after around $85$ epochs, the accuracy flattens and oscillates, stopping at $106$ epochs. Setting the epochs to $85$ may lead to a model that generalizes better.  "]},{"cell_type":"markdown","metadata":{},"source":["### Model assessment"]},{"cell_type":"markdown","metadata":{},"source":["Load the saved model. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# model = tf.keras.models.load_model(model_path)"]},{"cell_type":"markdown","metadata":{},"source":["We asses the model on the test dataset using `Model.evaluate`, which returns the loss and metrics values."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["loss_assess, accuracy_assess = model.evaluate(test_ds)\n","print('Test accuracy :', accuracy_assess)"]},{"cell_type":"markdown","metadata":{},"source":["Alternatively, we can load the data, predict and compute the desired metric."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load test data from test dataset. \n","def get_imgs_from_dataset(ds_test, ds_test_size):\n","    # Take images from data set ds_test: (data_test, data_test_noisy) \n","    data_test = []\n","    label_test = []\n","    count = 0\n","    for img, label in ds_test.take(ds_test_size):\n","        data_test_this = img.numpy()  \n","        label_test_this = label.numpy() \n","        if count == 0:\n","            data_test = data_test_this\n","            label_test = label_test_this\n","            count = 1\n","        else:            \n","            data_test = np.append(data_test, data_test_this, axis=0)\n","            label_test = np.append(label_test, label_test_this, axis=0)\n","    return data_test, label_test\n","    \n","# Test data\n","data_test, label_test = get_imgs_from_dataset(val_ds, len(val_ds)-1)"]},{"cell_type":"markdown","metadata":{},"source":["Predict and get accuracy on test set of $88$ %. This accuracy depends on the number of epochs, which can vary as we are using early stopping criterion. Higher number of epochs would improve accuracy, but we leave it for the fine-tuning step."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31171,"status":"ok","timestamp":1623158071546,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"ZwFGNsGiWAjn","outputId":"5d936f98-a49b-4bd5-ced6-58a7b73937c9"},"outputs":[],"source":["# Predict \n","data_pred = model.predict(data_test)\n","data_pred_class = np.argmax(data_pred, axis=1)\n","\n","acc_fn = tf.keras.metrics.Accuracy()\n","test_acc = acc_fn(data_pred_class, label_test)\n","print('Accuracy for %s with fine tuning is %.2f' % (base_model_name, test_acc))"]},{"cell_type":"markdown","metadata":{"id":"bSXk1YXbZIDe"},"source":["Save the trained model. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_name = f'{name_save}_{base_model_name}_TF{tf_vs}_ep{history.epoch[-1]}_FTat{fine_tune_at}_ep{history_fine.epoch[-1]}'\n","model.save(os.path.join(save_dir, f'{model_name}.keras'))\n","print(f\"Model saved in {os.path.join(save_dir, name_save)}\")\n","print(os.listdir(save_dir))"]},{"cell_type":"markdown","metadata":{"id":"ITRlneAPlOrX"},"source":["Save history."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vp7SlKcjiQdO"},"outputs":[],"source":["# Save history\n","with open(os.path.join(save_dir, f'{model_name}_history.json'), 'w') as file:\n","  json.dump(history_fine.history, file)"]},{"cell_type":"markdown","metadata":{"id":"NDaqF-nNZuYf"},"source":["## Final analysis and image exploration"]},{"cell_type":"markdown","metadata":{"id":"3CleENkor39B"},"source":["### Confusion matrix and display misclassifications"]},{"cell_type":"markdown","metadata":{"id":"DIYiJelbZ765"},"source":["To analyze the final results we build the confusion matrix. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":798,"status":"ok","timestamp":1622836378890,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"6GKNxR9_Z6jH","outputId":"f5b9b306-7176-4b2b-e663-82d09a09814c"},"outputs":[],"source":["# Confusion matrix\n","from sklearn.metrics import confusion_matrix\n","import pandas as pd\n","\n","def confusion_matrix_analysis(label_test, data_pred_class, class_names):\n","    confusion_mat = confusion_matrix(label_test, data_pred_class)\n","    confusion_df = pd.DataFrame(confusion_mat, columns=class_names, index=class_names)\n","    print('Confusion matrix (row: true, columns: predictions):')\n","    print(confusion_df)\n","    # Largest errors: roses as tulips, daisy as tulips, dandelion as tulips\n","    \n","    # Confusion matrix off-diagonals\n","    confusion_mat_off = np.copy(confusion_mat)\n","    np.fill_diagonal(confusion_mat_off, 0)\n","    confusion_as = np.sum(confusion_mat_off, axis=0)\n","    confusion_as_df = pd.Series(confusion_as, index=class_names)\n","    print('')\n","    print('Missclassified as ...')\n","    print(confusion_as_df)\n","    # flowers are misclassified mostly as tulips, \n","    # so tulips images must be revisited \n","    # dandelion presents the lowest (most data)\n","    \n","    confusion_for = np.sum(confusion_mat_off, axis=1)\n","    confusion_for_df = pd.Series(confusion_for, index=class_names)\n","    print('')\n","    print('Missclassifications for ...')\n","    print(confusion_for_df)\n","    # while tulips are the flower with lowest number of errors\n","    return confusion_df, confusion_as_df, confusion_for_df\n","\n","confusion_df, confusion_as_df, confusion_for_df = confusion_matrix_analysis(label_test, data_pred_class, class_names)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_confusion_matrix(cm, class_names):\n","    \"\"\"\n","    Returns a matplotlib figure containing the plotted confusion matrix.\n","\n","    Args:\n","        cm (array, shape = [n, n]): a confusion matrix of integer classes\n","        class_names (array, shape = [n]): String names of the integer classes\n","    \"\"\"\n","\n","    import itertools\n","\n","    figure = plt.figure(figsize=(8, 8))\n","    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n","    plt.title(\"Confusion matrix\")\n","    plt.colorbar()\n","    tick_marks = np.arange(len(class_names))\n","    plt.xticks(tick_marks, class_names, rotation=45)\n","    plt.yticks(tick_marks, class_names)\n","\n","    # Compute the labels from the normalized confusion matrix.\n","    labels = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=2)\n","\n","    # Use white text if squares are dark; otherwise black.\n","    threshold = cm.max() / 2.\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        color = \"white\" if cm[i, j] > threshold else \"black\"\n","        plt.text(j, i, labels[i, j], horizontalalignment=\"center\", color=color)\n","\n","    plt.tight_layout()\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","    return figure\n","\n","fig = plot_confusion_matrix(confusion_df.to_numpy(), class_names)\n","fig.savefig(os.path.join(save_dir, f\"{model_name}_confmat.png\"), bbox_inches='tight', dpi=300)"]},{"cell_type":"markdown","metadata":{"id":"e87bWkvjaL_q"},"source":["We get almost perfect classification for *tulips*. *Daisies* and *roses* are also well classified. *Sunflowers* and *dandelions* have the largest errors, which should be investigated. "]},{"cell_type":"markdown","metadata":{"id":"Ir5FF2BDbVry"},"source":["We display the misclassified results for analysis."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":813},"executionInfo":{"elapsed":3245,"status":"ok","timestamp":1623158112456,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"XvXMh8vnbeEX","outputId":"5edf70b2-d720-46ca-aba4-1c546eaaa5aa"},"outputs":[],"source":["# Explore misclassified results \n","def display_misclassified_images(data_test, label_test, data_pred_class, class_names, misclassified_only=True):\n","    test_misclassified = [i for i in range(len(label_test)) if (data_pred_class[i] != label_test[i])]\n","    classes_misclassified = []\n","    for class_ind, class_name in enumerate(class_names):\n","        label_test[test_misclassified] == class_ind\n","        class_misclassified = [i for i in range(len(label_test)) \n","            if (data_pred_class[i] != label_test[i])\n","            and (label_test[i] == class_ind)]\n","        classes_misclassified.append(class_misclassified)\n","        \n","        # Display misclassified images for each class\n","        print('Misclassified images for ' + class_name)\n","        plt.figure(figsize=(15, 15))\n","        for i in range(min(20,len(class_misclassified))):\n","            ax = plt.subplot(5, 4, i + 1)\n","            data_test_ind = class_misclassified[i]\n","            plt.imshow(data_test[data_test_ind].astype(\"uint8\"))\n","            plt.title(class_names[label_test[data_test_ind]] + ' as ' + class_names[data_pred_class[data_test_ind]],\n","                      fontsize=18)\n","            plt.axis(\"off\")\n","        plt.show()\n","        \n","        if misclassified_only is False:\n","            # Display well classified results for comparison\n","            class_well_classified = [i for i in range(len(label_test)) \n","                if (data_pred_class[i] == label_test[i])\n","                and (label_test[i] == class_ind)]\n","            print('Well classified images for ' + class_name)\n","            plt.figure(figsize=(15, 15))\n","            for i in range(min(20,len(class_well_classified))):\n","                ax = plt.subplot(5, 4, i + 1)\n","                data_test_ind = class_well_classified[i]\n","                plt.imshow(data_test[data_test_ind].astype(\"uint8\"))\n","                plt.title(class_names[label_test[data_test_ind]] + ' as ' + class_names[data_pred_class[data_test_ind]],\n","                          fontsize=18)\n","                plt.axis(\"off\")\n","            plt.show()\n","\n","# Display misclassified results\n","display_misclassified_images(data_test, label_test, data_pred_class, class_names)"]},{"cell_type":"markdown","metadata":{"id":"hbdQQiPDfNB8"},"source":["This display raises the following comments for further analysis: \n","* **Daisies**: The first image shows that daisies with pink color (not white) can be misclassified. The second image shows that a strong yellow background with reflections hardens the problem. \n","This could explained that are misclassified as tuilips. \n","\n","* **Dandelions**: Yellow dandelions are misclassified as sunflowers and white dandelions as daisies. \n","\n","* **Roses**: The first image is too hard. The others should be well classified (second for the background and the third one because of the zoom).\n","\n","* **Sunflowers**: Misclassification of sunflowers as roses \n","do not make sense; maybe, it is caused of the backgound. \n","Looking at the features that are activated may help. \n","One of the images does not have flower, which raises the question if the classifier is also learning the leave type for each class.\n","\n","* **Tulips**: the image seems wrong. We should check the test and training set to remove wrongly labeled images.\n","\n","Some of these misclassifications changed when re-training the model from scratch or using different base model, which encourages to use ensemble methods for futher accuracy. "]},{"cell_type":"markdown","metadata":{"id":"IwoYfJ9ntIhm"},"source":["### Asses on new acquired data"]},{"cell_type":"markdown","metadata":{"id":"lZ2PreQVtOSI"},"source":["We further assess the trained model on new images acquired with a mobile, download to /content/gdrive/MyDrive/Colab_Notebooks/Data "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GzbUrCwbsCBS"},"outputs":[],"source":["# Download new data\n","#dataset_url = \"https://www.dropbox.com/s/sofqjvmlt1omqr6/flowers_mobile.rar?dl=0\"\n","#data_new_dir = tf.keras.utils.get_file('flowers_mobile', origin=dataset_url, extract=True,)\n","data_new_dir = '/content/gdrive/MyDrive/Colab_Notebooks/Data'\n","os.chdir(data_new_dir)\n","!wget -P data_new_dir -O flowers_mobile.rar https://www.dropbox.com/s/sofqjvmlt1omqr6/flowers_mobile.rar?dl=0\n","!unrar flowers_mobile.rar -idq\n","!rm flowers_mobile.rar\n","data_new_dir = os.path.join(data_new_dir, 'flowers_mobile')\n","\n","print(f\"Dataset saved at: {data_new_dir}\")"]},{"cell_type":"markdown","metadata":{"id":"pfOVMrZX6oRp"},"source":["Define dataset and load images"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1965,"status":"ok","timestamp":1623159028563,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"VTkep3yC6mdn","outputId":"bade1eb9-7712-4f76-834a-fbff1e62d7f5"},"outputs":[],"source":["# New test data\n","test_new_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","  data_new_dir,\n","  validation_split=None,\n","  image_size=(img_height, img_width),\n","  batch_size=batch_size)\n","\n","data_test_new, label_test_new = get_imgs_from_dataset(test_new_ds, len(test_new_ds))"]},{"cell_type":"markdown","metadata":{"id":"zVCEFQHY5Kr5"},"source":["Classify new photos acquire with the mobile, even though images have been loaded upside down. Most images are well classified, including daisies out of focus. Photos with many daisies on green background are misclassied. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2591,"status":"ok","timestamp":1623159039077,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"qklVM7mQ5A5T","outputId":"49bf7be5-9e6e-4ab5-e6b9-ecf45e7e95a7"},"outputs":[],"source":["# Predict \n","data_pred_new = model.predict(data_test_new)\n","data_pred_new_class = np.argmax(data_pred_new, axis=1)\n","\n","acc_fn = tf.keras.metrics.Accuracy()\n","test_new_acc = acc_fn(data_pred_new_class, label_test_new)\n","print('Accuracy on new data is %.2f' % (test_new_acc))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":6458,"status":"ok","timestamp":1623159435356,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"e9uEkOe08q6T","outputId":"53a7f23c-751c-4e63-89ac-b0e9be004e8a"},"outputs":[],"source":["# Display misclassified images\n","display_misclassified_images(data_test_new, label_test_new, data_pred_new_class, class_names,misclassified_only=False)"]},{"cell_type":"markdown","metadata":{"id":"am_XRmBBfwWb"},"source":["### Conclusions and further analysis"]},{"cell_type":"markdown","metadata":{"id":"EE8MRyNGfmEE"},"source":["We have obtained an accurate classifier that reached $95$ % accuracy on a relatievely small data set using the 'Xception' model (we obtain $93$ % using the 'MobileNetV2' model using the same procedure; model saved in 'Results' folder). Training with VGG16, VGG19 and . Furter analysis should be considered for final accuracy quantification, maybe using generalized cross validation. \n","\n","Further work should focus on pursuing the misclassifications:\n","* From the results obtained, ensemble methods may lead to futher accuracy, as different models misclassified different images. \n","\n","* Images that were consistently misclassified, such as daisies, were due to color resembles to other classes, which could be improved. \n","\n","* Misclassifications due to zooms and backgrounds could be investigate by looking at activations of features. \n","\n","* Cleaning the data set and doing GCV could also help due the size of the dataset. \n","\n","* Working on the data rather than on the model seem to be a current approach to further improve results on the community. \n","\n","* It would be also nice to visualize the data using an embedding (eg. embedding projector on TF).\n","\n","An error table that includes the different possible sources of errors and their ocurrence should provide an ordered list of priorities. \n","\n"," "]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOG0nbgky1lXJjAzWKH+bAi","name":"image_classification_using_transfer_learning.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
