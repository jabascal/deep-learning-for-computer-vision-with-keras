{"cells":[{"cell_type":"markdown","metadata":{},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jabascal/deep-learning-for-computer-vision-with-keras/blob/main/adversarial_attacks_and_adversarial_training.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"HxNmcxcnuQSg"},"source":["# Adversarial attacks and adversarial training"]},{"cell_type":"markdown","metadata":{"id":"vmEOncRtr1ms"},"source":["## Introduction"]},{"cell_type":"markdown","metadata":{"id":"g9lpY7dC3GCq"},"source":["It is well known that *adversarial attacks* can make state-of-the art classifiers to consistently fail based on a optimization procedure that maximises its prediction error by slightly changing a data point such that it is related to a very different class (Szegedy 2014; Goodfellow 2014). Some of these image manipulations are so small at each pixel that are not distinguisable by eye and are consistent across different architectures and trained on different sets of examples. \n","\n","Goodfellow showed that these adversarial examples could be explained by an excessive linearity of the mapping from the input to the output of the model, as neural networks (NN) can be considered as piecewise constant functions (Goodfellow 2014). We note that the map from parameters of the NN to the output is nonlinear: this makes problems optimizing on the input of the model easier than fitting the model parameters. NN are not the only models sensitive to these type of attacks, as other models such as logistic regression, linear SVMs, decision trees, and neareast neighbors failed under these adversarial attacks in a similar way (Goodfellow, 2017). The facts that different methods lead to similar misclassification with the same adversarial examples can be explained by the hypothesis that they are learning similar linear classifiers (Goodfellow 2014)."]},{"cell_type":"markdown","metadata":{"id":"ni0m1c88GHXr"},"source":["These results go against previous assumptions on neural nets. It has been assumed that NNs constitute *non-local generalization priors* over the input space. In this case, regions of the space with no training examples will get low probabilities. Let $x\\in\\mathbb{R}^p$ be an image and $r$ a small perturbation with $\\|r\\|<\\epsilon$, *local* generalizations will make perturbed examples $x+r$ to get assigned the same class as $x$ (Szegedy 2014). Finding adversarial examples shows that this smoothness assumption does not hold and that the manifold represented by the network can be crossed using optimization, finding adversarial examples. Training with a cross entropy loss can be explained by learning the conditional probability of a class given an input. Then, adversarial examples lie in low-probability regions of the manifold. This also goes against the tendency of using CNNs for feature representation where Euclidean distance approximates perceptual distance, as images with small perceptual distance can lead to completely different classes."]},{"cell_type":"markdown","metadata":{"id":"hQlYV97mQsdk"},"source":["On the other hand, *adversarial training* can alter this linear behavior by encouraging locally constant behavior when making it robust to these perturbations. Thus, adversarial training may improve \n","generalization and can provide regularization effects further different to those of dropout (Goodfellow 2014). This is motivated by the fact that natural images classes lie on disconnected manifolds. When classes are well separated, we expect that a classifier will assign the same class to two inputs $x$ and $\\tilde{x}$ as long as $\\|x-\\tilde{x}\\|_{\\infty}$. \n","\n","This type of augmented data training is different to the commonly data augmentation (input transformations), which are highly correlated and belong to the same probability distribution. A point in favor of NN is that while adversarial examples affect linear methods and other methods such as nearest neighbors, NN can learn any nonlinear function and can be trained to resist these attacks. However, shallow networks can not be fit to be constant around training points (Goodfellow 2014)."]},{"cell_type":"markdown","metadata":{"id":"Csfi-oRivHzl"},"source":["Recent theoretical works aim at understanding the origin of adversarial examples and the effect of adversarial training. (Allen-Zhu 2020) shows that learning leads to an accumulation of small mixtures of the hidden weights, as gradient descent updates are highly correlated, and that adversarial training purifies this weights  affecting the learned features, making them to resemble more the original image space. \n","\n","Adversarial training at pretraining and fine-tuning stages, in combination with adversarial perturbation of the embedidng space (image features), has been proposed for large scale boosting performance for vision and language applications (Gan 2020). "]},{"cell_type":"markdown","metadata":{"id":"GWKrzheFGGgr"},"source":["In this repository, we explore several adversarial attack methods and their effect on misclassification. "]},{"cell_type":"markdown","metadata":{"id":"6g_U1A15zRxH"},"source":["## Notebook set-up"]},{"cell_type":"markdown","metadata":{"id":"PKW4h1vmvaXy"},"source":["### Dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1960,"status":"ok","timestamp":1622805663433,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"nfCjPDRdvc1a"},"outputs":[],"source":["import os\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import PIL\n","import tensorflow as tf\n","from tensorflow import keras "]},{"cell_type":"markdown","metadata":{"id":"WduIJ2VUhfrx"},"source":["### Mount google drive and paths"]},{"cell_type":"markdown","metadata":{"id":"o80j4-WHFpm9"},"source":["Start by mounting your google drive:   "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16037,"status":"ok","timestamp":1622805679467,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"bVqNrmTrFnkU","outputId":"cdf399c5-968a-475e-c48e-49a3bb2c070a"},"outputs":[],"source":["# Mount google drive to access files via colab\n","mode_colab = True\n","if mode_colab:\n","    from google.colab import drive\n","    drive.mount(\"/content/gdrive\")"]},{"cell_type":"markdown","metadata":{"id":"LEZxDieMutPP"},"source":["Paths for saving results"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1622805679468,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"IRq5FX3jrmlB","outputId":"616ddc20-0812-4d48-af2e-d39ad54e6fc1"},"outputs":[],"source":["name_save = \"adv_train\"\n","\n","# Path results\n","if mode_colab:\n","      save_dir = f\"/content/gdrive/MyDrive/Colab_Notebooks/Results/{name_save}\"\n","      data_dir = '/content/gdrive/MyDrive/Colab_Notebooks/Data'\n","else:\n","      save_dir = f\"../../Results/{name_save}\"\n","      data_dir = '../../Data'\n","if os.path.exists(save_dir) is False:\n","      os.mkdir(save_dir)\n","      print(f\"Directory: {save_dir} created.\")\n","if os.path.exists(data_dir) is False:\n","      os.mkdir(data_dir)\n","      print(f\"Directory: {data_dir} created.\")"]},{"cell_type":"markdown","metadata":{"id":"kzW7t52ITYuE"},"source":["## Load model and classify"]},{"cell_type":"markdown","metadata":{"id":"b1GAUr-GTrHL"},"source":["Load pretrained keras model MobileNetV2 (Sandler 2018), trained on ImageNet, and ImageNet labels: "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2133,"status":"ok","timestamp":1622805681598,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"NFs8H-D-TsTZ","outputId":"027ec6f6-02ba-4096-f826-b39f34a838d4"},"outputs":[],"source":["# Pretrained model\n","model = tf.keras.applications.MobileNetV2(include_top=True,\n","                                                     weights='imagenet')\n","# Freeze wights\n","model.trainable = False\n","\n","# ImageNet probs to class and confidence\n","# _, image_class, class_confidence = decode_predictions(probs, top=1)[0][0]\n","decode_predictions = tf.keras.applications.mobilenet_v2.decode_predictions\n","\n","model_preprocess = tf.keras.applications.mobilenet_v2.preprocess_input"]},{"cell_type":"markdown","metadata":{"id":"QSCAaMPO5lae"},"source":["Define preprocessing, labeling and displaying functions as defined in https://www.tensorflow.org/tutorials/generative/adversarial_fgsm"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1622805681598,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"-I1pqpA-5t2W"},"outputs":[],"source":["# Preprocess the image for MobileNetV2: Resize and normalize\n","def preprocess_image(image, model_preprocess):\n","  # tf.uint8 -> tf.float32\n","  image = tf.cast(image, tf.float32)\n","  image = tf.image.resize(image, (224, 224))\n","  # Normalize to [-1,1]\n","  image = model_preprocess(image)\n","  image = image[None, ...]\n","  return image\n","\n","# Get predicted probability output\n","def get_imagenet_label(probs, decode_predictions):\n","  # Return 1 prediction from (1, 1000) vector of probabilities\n","  _, image_class, class_confidence = decode_predictions(probs, top=1)[0][0]\n","  return image_class, class_confidence\n","\n","# Display image and classification output\n","def display_img_prediction(image, image_class, class_confidence):\n","  # image tf tensor [1, 224, 224, 3]\n","  fig = plt.figure()\n","  plt.imshow(image[0]*0.5+0.5)\n","  result = '{} : {:.2f}% Confidence'.format(image_class, class_confidence*100)\n","  print(result)\n","  plt.title(result)\n","  plt.axis('off')\n","  plt.show()\n","  return fig \n","\n","def normalize_back_img(img):\n","  # Normalize from [-1,1] to [0,255]\n","  return 255*(img + 1)/2   "]},{"cell_type":"markdown","metadata":{"id":"RPL4sC_T5YOP"},"source":["Load image example and preprocess it"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1622805681599,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"btzQsYBp4akF","outputId":"3e2b58d1-41ce-4593-bcf1-8134ba1982e8"},"outputs":[],"source":["# Download image\n","test_image_path = tf.keras.utils.get_file('YellowLabradorLooking_new.jpg', 'https://storage.googleapis.com/download.tensorflow.org/example_images/YellowLabradorLooking_new.jpg')\n","test_image_raw = tf.io.read_file(test_image_path)\n","test_image = tf.image.decode_image(test_image_raw)\n","\n","# Cast to float, reshape and normalizes to [-1, 1]\n","test_image = preprocess_image(test_image, model_preprocess)"]},{"cell_type":"markdown","metadata":{"id":"bfODosUB8WZ8"},"source":["Classify test image and display result"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":353},"executionInfo":{"elapsed":1824,"status":"ok","timestamp":1622805683420,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"YY1Uy4HjYBtS","outputId":"0972febc-aceb-45df-d7b4-ae09bd3a623d"},"outputs":[],"source":["# Predict\n","test_pred = model.predict(test_image)\n","test_pred_class, test_pred_confidence = get_imagenet_label(test_pred, decode_predictions)\n","fig = display_img_prediction(test_image, test_pred_class, test_pred_confidence)\n","fig.savefig(os.path.join(save_dir, \"test_image_pred.png\"))"]},{"cell_type":"markdown","metadata":{"id":"7w4vaadDbx-U"},"source":["## Adversarial attacks"]},{"cell_type":"markdown","metadata":{"id":"aYSvqGrVViXd"},"source":["### Adversarial attack using FGSM method"]},{"cell_type":"markdown","metadata":{"id":"Ebs0xxNLVx0M"},"source":["Adversarial attacks solve the following optimization problem:\n","\n","$$\n","\\min_r f(x+r)=l \\quad \\textrm{s.t.} \\quad (x+r)\\in D,\n","$$\n","\n","where $f$ is the classifier, $x$ is the input image and $r$ is a perturbation that makes the adversarial example $x+r$ to be incorrectly classified as $l\\neq h(x)$. \n","\n","Fast gradient sign method (FGSM) solves the previous problem by taking an step along the gradient of the cost function of the classifier $J$ with respect to the input image: \n","\n","$$\n","\\tilde{x}=x+\\epsilon \\textrm{sign}(\\nabla_x J(x,y_{\\textrm{true}})). \n","$$"]},{"cell_type":"markdown","metadata":{"id":"hhq1GKG96hu8"},"source":["Given the weights vector $w$ and a perturbation defined as $r=\\textrm{sign}(w)$ for $w^T\\tilde{x}=w^Tx+w^Tr$,  the *sign* function maximizes the perturbation while ensuring the constraint $\\|r\\|_{\\infty}\\leq\\epsilon$. If $x$ has $p$ dimensions and the elements of $w$ have an average value of $m$, then the changes in the activation $w^Tr$ can grow linearly with $p$ and as much as $\\epsilon pm$. Thus, high-dimensional linear models are vulnerable to adversarial examples and NNs are \"too linear to resist linear adversarial pertubation\" (Goodfellow 2014). "]},{"cell_type":"markdown","metadata":{"id":"z_j8gut8Tlwo"},"source":["To compute the adversarial perturbation $r$, we define the loss $J$ and compute its gradient $\\nabla_x J$ wrt the input image $x$ and its label $y_{\\textrm{true}}$."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":187,"status":"ok","timestamp":1622805780159,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"VQnNKSSMVlHL"},"outputs":[],"source":["# Define the loss\n","loss_object = tf.keras.losses.CategoricalCrossentropy()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":192,"status":"ok","timestamp":1622805802916,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"D8kFwTmHdhou"},"outputs":[],"source":["def adversarial_perturbation(img, label):\n","  # Compute the gradient of the loss wrt the input image\n","  with tf.GradientTape() as tape:\n","    tape.watch(img)\n","    img_pred = model(img)\n","    loss = loss_object(label, img_pred)\n","  gradient = tape.gradient(loss, img)\n","\n","  # Create perturbation as the gradient sign \n","  gradient_sign = tf.sign(gradient)\n","  return gradient_sign"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":539,"status":"ok","timestamp":1622805805495,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"kdOHl-b5fL94"},"outputs":[],"source":["# One hot labels vector (1,1000)\n","test_image_index = 208\n","num_classes = test_pred.shape[-1]\n","test_one_hot_label = tf.one_hot(test_image_index, num_classes)\n","test_one_hot_label = tf.reshape(test_one_hot_label, (1, num_classes))\n","\n","# Create adversarial perturbation\n","img_perturbation = adversarial_perturbation(test_image, test_one_hot_label)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":335},"executionInfo":{"elapsed":915,"status":"ok","timestamp":1622805809340,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"thi7LviuhbFq","outputId":"7843ea6d-705e-459e-96cb-fd9f2667454c"},"outputs":[],"source":["# Display the perturbation\n","fig = plt.figure()\n","plt.imshow(img_perturbation[0])\n","plt.colorbar()\n","plt.title('Adversarial perturbation')\n","plt.axis('off')\n","plt.savefig(os.path.join(save_dir, \"fgsm_perturbation.png\"))"]},{"cell_type":"markdown","metadata":{"id":"HAbQLHWvX4bG"},"source":["Advesarial attacks where no pixel is perturbed in more than $1$, ie. $|r_i|\\leq 1$."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":193,"status":"ok","timestamp":1622805813584,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"tUyZxYzOkhF-"},"outputs":[],"source":["def normalize_back_img(img):\n","  # Normalize from [-1,1] to [0,255]\n","  return 255*(img + 1)/2   "]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":206,"status":"ok","timestamp":1622806103881,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"ylg3J_yXh3Ay"},"outputs":[],"source":["# Adversarial attack\n","def adversarial_attack(img,img_perturbation, eps=0.9):\n","  img_adversarial = (normalize_back_img(img) + eps*img_perturbation) \n","  img_adversarial = 255*img_adversarial/np.max(img_adversarial)\n","  return img_adversarial\n","\n","test_adversarial = adversarial_attack(test_image,img_perturbation, eps=0.9)"]},{"cell_type":"markdown","metadata":{"id":"Bz--Rd8BYPD4"},"source":["Display the adversarial corrupted image and classify it, providing a confidence value."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":298},"executionInfo":{"elapsed":961,"status":"ok","timestamp":1622806110266,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"5UMziBW2h21S","outputId":"89fd88ee-221f-46c3-b4c0-f1fc54b7340d"},"outputs":[],"source":["# Process image as required by the model\n","test_adversarial = preprocess_image(test_adversarial[0], model_preprocess)\n","# Predict probabilities for each class\n","test_adversarial_pred = model.predict(test_adversarial)\n","# Class and confidence\n","test_adversarial_class, test_adversarial_class_confidence = get_imagenet_label(test_adversarial_pred, decode_predictions)\n","# Display\n","fig = display_img_prediction(test_adversarial, test_adversarial_class, test_adversarial_class_confidence)\n","fig.savefig(os.path.join(save_dir, \"fgsm_pred.png\"))"]},{"cell_type":"markdown","metadata":{"id":"ZK-81wTnsAcE"},"source":["Mission achieved: The classifier predicts the wrong class! However, the confidence is low!\n","\n","Next we assess the effect of the size of the perturbation on misclassified responses."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2376,"status":"ok","timestamp":1622806345367,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"qP7fZCKXsXec"},"outputs":[],"source":["# Size of perturbation\n","eps_all = np.linspace(0.001, 0.9, 30)\n","test_adversarial_class_all = []\n","test_adversarial_class_confidence_all = []\n","for eps in eps_all:\n","  # Adversarial attack\n","  test_adversarial = adversarial_attack(test_image,img_perturbation, eps=eps)\n","  test_adversarial = preprocess_image(test_adversarial[0], model_preprocess)  \n","  test_adversarial_pred = model.predict(test_adversarial)\n","  test_adversarial_class, test_adversarial_class_confidence = get_imagenet_label(test_adversarial_pred, decode_predictions)\n","  \n","  test_adversarial_class_all.append(test_adversarial_class)\n","  test_adversarial_class_confidence_all.append(test_adversarial_class_confidence)\n","\n","  #print('Perturbation size %f:' %(eps) )\n","  #display_img_prediction(test_adversarial, test_adversarial_pred)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":296},"executionInfo":{"elapsed":724,"status":"ok","timestamp":1622806456921,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"8lbi1Mkqt8nW","outputId":"33299bdb-5f22-4c96-ddb7-0fd75b82eef4"},"outputs":[],"source":["# Misclassifications vs eps\n","true_clas = [test_adversarial_class_all[i] == 'Labrador_retriever' for i in range(len(test_adversarial_class_all))]\n","\n","fig = plt.figure(figsize=(10, 4))\n","plt.subplot(1,2,1)\n","plt.plot(eps_all, true_clas)\n","plt.ylabel('True classification')\n","plt.xlabel('eps')\n","#\n","# Confidence vs eps\n","plt.subplot(1,2,2)\n","plt.plot(eps_all, test_adversarial_class_confidence_all)\n","plt.ylabel('Confidence')\n","plt.xlabel('eps')\n","fig.savefig(os.path.join(save_dir, \"fgsm_vs_perturb.png\"))"]},{"cell_type":"markdown","metadata":{"id":"cy3DRquKxGW8"},"source":["The size of the perturbation must be larger than $0.25$ for the adversarial attack to be successful. However, the confidence is low, so the adversarial attack may not go unnoticed. For instance, for $0.25$ the confidence interval is only $9.27$%. \n","\n","Largest perturbations lead to higher confidence, yet they are more prone to fail. Perturbation of size $5$ leads to $48$ % confidence. "]},{"cell_type":"markdown","metadata":{"id":"VLR0uX676FdR"},"source":["### Target Class Method"]},{"cell_type":"markdown","metadata":{"id":"ycOMIrGr6ULi"},"source":["We assess the target class method in order to obtain higher confidence values, which may make the attack to go unnoticed (Kurakin 2016).  \n","\n","It is a modification of FGSM that computes the gradient of the cost for a 'target class' instead of the input image class as in the original FGSM method: \n","\n","$$\n","\\tilde{x}=x-\\epsilon \\textrm{sign}(\\nabla_x J(x,y_{\\textrm{target}})). \n","$$"]},{"cell_type":"markdown","metadata":{"id":"ZwihfNkK7ckr"},"source":["We define below the perturbation and the attack for the target method."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":188,"status":"ok","timestamp":1622808201213,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"VriYzaHEz77z"},"outputs":[],"source":["# Target method\n","def adversarial_perturbation_target_label(img, label_target):\n","  # Compute the gradient of the loss wrt the input image\n","  with tf.GradientTape() as tape:\n","    tape.watch(img)\n","    img_pred = model(img)\n","    loss = loss_object(label_target, img_pred)\n","  gradient = tape.gradient(loss, img)\n","  # Create perturbation as the gradient sign \n","  gradient_sign = tf.sign(gradient)\n","  return gradient_sign\n","\n","# Adversarial attack\n","def adversarial_attack_target_label(img,img_perturbation, eps=0.9):\n","  img_adversarial = (normalize_back_img(img) - eps*img_perturbation) \n","  img_adversarial = 255*img_adversarial/np.max(img_adversarial)\n","  return img_adversarial\n","\n"]},{"cell_type":"markdown","metadata":{"id":"hGUXLXYc7g56"},"source":["We need to select a target class. We have seen that selecting specific classes leads to higher confidence, as it is the case of 'Siberian husky' and 'Saluki, gazelle hound'. This could be because of the related shape and color. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":580},"executionInfo":{"elapsed":1938,"status":"ok","timestamp":1622808212266,"user":{"displayName":"JUAN FELIPE PEREZ JUSTE ABASCAL","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghd5R2-WK1mTx-WYxDlXWzLj0Ht7HiyYusU4TEEIg=s64","userId":"18272978571042410721"},"user_tz":-120},"id":"SVzoOkqk0Ndy","outputId":"76bbdbe3-7473-4afd-b0d2-899b8fe19991"},"outputs":[],"source":["# Target german shepherd\n","# 235: 'German shepherd, German shepherd dog, German police dog, alsatian',\n","# 250: 'Siberian husky',\n","# 251: 'dalmatian, coach dog, carriage dog',\n","# test_one_hot_label = tf.one_hot(250, num_classes)\n","# 176: 'Saluki, gazelle hound',\n","# 290: 'jaguar, panther, Panthera onca, Felis onca',\n","# 250, 176\n","label_targets = (250, 176)\n","for label_target in label_targets:\n","  test_one_hot_label = tf.one_hot(label_target, num_classes)\n","  test_one_hot_label = tf.reshape(test_one_hot_label, (1, num_classes))\n","\n","  # Modified adversarial attack\n","  img_perturbation_mod = adversarial_perturbation_target_label(test_image, test_one_hot_label)\n","  test_adversarial = adversarial_attack_target_label(test_image,img_perturbation_mod, eps=0.9)\n","  test_adversarial = preprocess_image(test_adversarial[0], model_preprocess)\n","  test_adversarial_pred = model.predict(test_adversarial)\n","  test_adversarial_class, test_adversarial_class_confidence = get_imagenet_label(test_adversarial_pred, decode_predictions)\n","  fig = display_img_prediction(test_adversarial, test_adversarial_class, test_adversarial_class_confidence)\n","  fig.savefig(os.path.join(save_dir, f\"adv_targetclass_pred_class_{label_target}.png\"))"]},{"cell_type":"markdown","metadata":{"id":"2WMWbfzz9uOj"},"source":["'Saluki, gazelle hound' with $90.12$% confidence, not bad!"]},{"cell_type":"markdown","metadata":{"id":"VRObonL2GBwC"},"source":["### More on adversarial training"]},{"cell_type":"markdown","metadata":{"id":"aAvB8NpuGY2a"},"source":["Other methods exist for creating adversarial examples. Rotating slightly the image $x$ in the direction of the previous gradient also yields adversarial examples (Goodfellow 2014). Other strategies have been proposed to counteract some of these attacks, but still none of them can mitigate all these perturbations (Chakraborty, 2018). An iterative method based on FGSM \n","\n","$$\n","x^0=x, \\quad x^{n+1}=Clip_x(x^n+\\epsilon \\textrm{sign}(\\nabla_x J(x,y_{\\textrm{true}}))), \n","$$\n","\n","where clipping makes their values to belong to the interval $[x-\\epsilon,x+\\epsilon]$.\n","\n","Another approach to *adversarial training* proposes an adversarial objective function that acts as a regularizer:\n","\n","$$\n","\\tilde{J}(w,x,y)=\\alpha J(w,x,y)+(1-\\alpha)J(w,x+\\epsilon\\textrm{sign}\\nabla_x J(w,x,y),y),\n","$$\n","\n","where $\\alpha=0.5$. With this training, the error rate on adversarial examples fell from $89.4$% to $17.9$%, by using a deep network. 'Adversarial training procedure can be seen as minimizing the worst case error' (Goodfellow 2014). "]},{"cell_type":"markdown","metadata":{"id":"fC6lMZxwom04"},"source":["A recent result proved that adversarial training makes the model robust to similar attacks and that low complexity models such as linear classifiers are not able to defend against these attacks (Allen-Zhu 2020)."]},{"cell_type":"markdown","metadata":{"id":"mwqLSLRCzIuy"},"source":["## Conclusions"]},{"cell_type":"markdown","metadata":{"id":"wqzngPTLzM1i"},"source":["We have created adversarial attacks based on the original FGSM method and the target class method. The latter can lead to higher confidence attacks.  More sophisticated adversarial attacks may be used, as methods that are computationally expensive to evaluate may be hard to use in adversarial training strategies."]},{"cell_type":"markdown","metadata":{"id":"5yeBbCrn9FK5"},"source":["## References"]},{"cell_type":"markdown","metadata":{},"source":["This repository example is based on the following resources: \n","\n","* [Keras adversarial example](https://www.tensorflow.org/tutorials/generative/adversarial_fgsm) using FGSM.\n","* [Lecture 16 - Adversarial Examples and Adversarial Training](https://www.youtube.com/watch?v=CIfsB_EYsVI), \n","Stanford University School of Engineering, 2017. "]},{"cell_type":"markdown","metadata":{"id":"wxBMH-nm9IJx"},"source":["Other references: \n","\n","* (Allen-Zhu 2020) Z Allen-Zhu and Y Li, Feature Purification: How Adversarial Training Performs Robust Deep Learning, 2020, arXiv:2005.10190v2\n","* (Chakraborty, 2018) A Chakraborty et al, Adversarial Attacks and Defences: A Survey, 2018\n","* (Gan 2020) Z Gan et al, Large-Scale Adversarial Training for\n","Vision-and-Language Representation Learning, 2020, arXiv:2006.06195v2\n","* (Goodfellow 2016) I Goodfellow et al, Deep learning (Chapter 7), MIT, 2016\n","* (Kurakin 2016) Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. 2016. Adversarial Machine Learning at Scale, arXiv:1611.01236\n","* (Sandler 2018) M Sandler et al, MobileNetV2: Inverted Residuals and Linear Bottlenecks, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2018, pp. 4510-4520.\n","* (Szegedy 2014) Szegedy, Christian, Zaremba, Wojciech, Sutskever, Ilya, Bruna, Joan, Erhan, Dumitru, Goodfellow, Ian J., and Fergus, Rob. Intriguing properties of neural networks. ICLR, abs/1312.6199, 2014.\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPcSH+TGDAgthnOhcSYGIE9","name":"adversarial_attacks_and_adversarial_training.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
